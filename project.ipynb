{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "suburban-issue",
   "metadata": {},
   "source": [
    "### Group C\n",
    "- Campos, Joshua\n",
    "- Halili, Gesara\n",
    "- Nwuzor, Chisom\n",
    "- Tran, Quynh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "roman-conspiracy",
   "metadata": {},
   "source": [
    "# Data Science Project 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prime-pattern",
   "metadata": {},
   "source": [
    "In this project, we will be tackling a supervised classification problem using the data from the Kaggle competition \"[Two Sigma Connect: Rental Listing Inquiries](https://www.kaggle.com/c/two-sigma-connect-rental-listing-inquiries)\". Our task will be to classify rental listing inquiries, and predict how much interest they will receive according to three categories: Low, Medium and High. \n",
    "\n",
    "For this project, we will be using three different models, and comparing their performance to find the best one for this specific task. The models will be:\n",
    "- Regularized Linear Models\n",
    "- Trees\n",
    "- Random Forests\n",
    "\n",
    "Our classification problem will be composed of various sections: \n",
    "- Preparation & Exploratory Data Analysis\n",
    "- Data Preprocessing & Feature Engineering\n",
    "- Model Performance & Hyper-Parameter Tuning\n",
    "- Prediction Explanation & Story Telling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fleet-pattern",
   "metadata": {},
   "source": [
    "## Preparation & Exploratory Data Analysis\n",
    "\n",
    "### Importing the libraries\n",
    "\n",
    "The first thing we have to do is to import all the libraries that we will be using for our project, which include the typical libraries for data science, such as Numpy, Pandas, Matplotlib, and Scikit-Learn, among others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "specified-vaccine",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import string \n",
    "\n",
    "from re import search \n",
    "\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, GridSearchCV, RepeatedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import OneHotEncoder, MultiLabelBinarizer, LabelEncoder, OrdinalEncoder\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "import functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "existing-guinea",
   "metadata": {},
   "source": [
    "### Reading the file\n",
    "\n",
    "Here we read the data, which is stored as a JSON file. We will be only using the training data for the whole project, which we will split into training and testing sets respectively. This is done to decrease the processing power and time required for the model training and the hyper-parameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "violent-harris",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/train.json') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "flexible-compensation",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis\n",
    "\n",
    "Before continuing, we must get a grasp of what the data looks like. For this, we print the head of the data, as well as some statistics and information about the variables. Besides this, we also take a look at the unique number of values for the display addresses, the manager ids, and the features. \n",
    "\n",
    "Finally, we encode the target variable from text to numerical values, ranging from 0 to 2, and plot the counts for each class to get an idea of how balanced the dataset is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rapid-clinic",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_head = df.head()\n",
    "df_describe = df.describe()\n",
    "df_info = df.info()\n",
    "\n",
    "print('\\nNum. of Unique Display Addresses: {}'.format(df['display_address'].nunique()))\n",
    "print('Num. of Unique Manager IDs: {}'.format(df['manager_id'].nunique()))\n",
    "\n",
    "all_unique_features = set()    \n",
    "df['features'].apply(lambda x: functions.add_unique_elements(x, all_unique_features))\n",
    "print('Num. of Unique Features: {}'.format(len(all_unique_features)))\n",
    "\n",
    "\n",
    "df['interest_level'] = df['interest_level'].apply(lambda x: 0 if x=='low' else 1 if x=='medium' else 2)\n",
    "# Check homogeneity of target values\n",
    "sns.countplot('interest_level', data=df)\n",
    "plt.title('Unbalanced Classes')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "animated-gnome",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "environmental-petite",
   "metadata": {},
   "source": [
    "#### Categorical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "educational-insert",
   "metadata": {},
   "outputs": [],
   "source": [
    "addresses = ['display_address', 'street_address']\n",
    "\n",
    "for address in addresses:\n",
    "    print(address)\n",
    "    ''' delete rows that contain descriptions instead of actual addresses '''\n",
    "    address_delete = [] \n",
    "    for i in range(len(df)):\n",
    "        address_val = df[address][i]\n",
    "        if search('!' or '*', address_val):\n",
    "            address_delete.append(i)\n",
    "\n",
    "    df = df.drop(df.index[address_delete])\n",
    "    print(\"Num. of deleted addresses: {}\".format(\n",
    "        len(address_delete)))\n",
    "    \n",
    "    \n",
    "    ''' Data Cleaning '''\n",
    "    address_column = df[address]\n",
    "    address_column_transformed = ( address_column\n",
    "                                           .apply(str.upper)\n",
    "                                           .apply(lambda x: x.replace('WEST','W'))\n",
    "                                           .apply(lambda x: x.replace('EAST','E'))\n",
    "                                           .apply(lambda x: x.replace('STREET','ST'))\n",
    "                                           .apply(lambda x: x.replace('AVENUE','AVE'))\n",
    "                                           .apply(lambda x: x.replace('BOULEVARD','BLVD'))\n",
    "                                           .apply(lambda x: x.replace('.',''))\n",
    "                                           .apply(lambda x: x.replace(',',''))\n",
    "                                           .apply(lambda x: x.replace('&',''))\n",
    "                                           .apply(lambda x: x.replace('(',''))\n",
    "                                           .apply(lambda x: x.replace(')',''))\n",
    "                                           .apply(lambda x: x.strip())\n",
    "                                           #.apply(lambda x: re.sub('(?<=\\d)[A-Z]{2}', '', x))\n",
    "                                           .apply(lambda x: re.sub('[^A-Za-z0-9]+ ', '', x)) #remove all special characters and punctuaction\n",
    "                                           .apply(lambda x: x.replace('FIRST','1'))\n",
    "                                           .apply(lambda x: x.replace('SECOND','2'))\n",
    "                                           .apply(lambda x: x.replace('THIRD','3'))\n",
    "                                           .apply(lambda x: x.replace('FOURTH','4'))\n",
    "                                           .apply(lambda x: x.replace('FIFTH','5'))\n",
    "                                           .apply(lambda x: x.replace('SIXTH','6'))\n",
    "                                           .apply(lambda x: x.replace('SEVENTH','7'))\n",
    "                                           .apply(lambda x: x.replace('EIGHTH','8'))\n",
    "                                           .apply(lambda x: x.replace('EIGTH','8'))\n",
    "                                           .apply(lambda x: x.replace('NINTH','9'))\n",
    "                                           .apply(lambda x: x.replace('TENTH','10'))\n",
    "                                           .apply(lambda x: x.replace('ELEVENTH','11'))\n",
    "                                         )\n",
    "\n",
    "    print(\"Num. of Unique Addresses after Transformation: {}\".format(\n",
    "        address_column_transformed.nunique()))\n",
    "\n",
    "    df[address] = address_column_transformed \n",
    "    \n",
    "\n",
    "display=df[\"display_address\"].value_counts()\n",
    "manager_id=df[\"manager_id\"].value_counts()\n",
    "building_id=df[\"building_id\"].value_counts()\n",
    "street=df[\"street_address\"].value_counts()\n",
    "\n",
    "df[\"display_count\"]=df[\"display_address\"].apply(lambda x:display[x])\n",
    "df[\"manager_count\"]=df[\"manager_id\"].apply(lambda x:manager_id[x])  \n",
    "df[\"building_count\"]=df[\"building_id\"].apply(lambda x:building_id[x])\n",
    "df[\"street_count\"]=df[\"street_address\"].apply(lambda x:street[x])\n",
    "\n",
    "price_by_building = df.groupby('building_id')['price'].agg([np.min,np.max,np.mean]).reset_index()\n",
    "price_by_building.columns = ['building_id','min_price_by_building',\n",
    "                            'max_price_by_building','mean_price_by_building']\n",
    "df = pd.merge(df,price_by_building, how='left',on='building_id')\n",
    "df = df.drop(df.index[address_delete])\n",
    "\n",
    "cat_vars = ['building_id','manager_id','display_address','street_address']\n",
    "OE = OrdinalEncoder()\n",
    "for cat_var in cat_vars:\n",
    "    print (\"Ordinal Encoding %s\" % (cat_var))\n",
    "    df[cat_var]=OE.fit_transform(df[[cat_var]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "contained-vinyl",
   "metadata": {},
   "source": [
    "#### Text Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "crucial-dodge",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Studies have shown that titles with excessive all caps and special characters give renters the impression \n",
    "# that the listing is fraudulent â€“ i.e. BEAUTIFUL***APARTMENT***CHELSEA.\n",
    "df['num_of_#']=df.description.apply(lambda x:x.count('#'))\n",
    "df['num_of_!']=df.description.apply(lambda x:x.count('!'))\n",
    "df['num_of_$']=df.description.apply(lambda x:x.count('$'))\n",
    "df['num_of_*']=df.description.apply(lambda x:x.count('*'))\n",
    "df['num_of_>']=df.description.apply(lambda x:x.count('>'))\n",
    "\n",
    "df['has_phone'] = df['description'].apply(lambda x:re.sub('['+string.punctuation+']', '', x).split())\\\n",
    "        .apply(lambda x: [s for s in x if s.isdigit()])\\\n",
    "        .apply(lambda x: len([s for s in x if len(str(s))==10]))\\\n",
    "        .apply(lambda x: 1 if x>0 else 0)\n",
    "df['has_email'] = df['description'].apply(lambda x: 1 if '@renthop.com' in x else 0)\n",
    "\n",
    "display_address_column = df['description']\n",
    "df['description'] = [functions.text_cleaner(x) for x in display_address_column]\n",
    "\n",
    "df['length_description'] = df['description'].apply(lambda x: len(x))\n",
    "df['num_words_description'] = df['description'].apply(lambda x: len(x.split(\" \")))\n",
    "\n",
    "df['num_features'] = df['features'].apply(len)\n",
    "\n",
    "v = CountVectorizer(stop_words='english', max_features=50)\n",
    "x = v.fit_transform(df['features']\\\n",
    "                                     .apply(lambda x: \" \".join([\"_\".join(i.split(\" \")) for i in x])))\n",
    "\n",
    "df1 = pd.DataFrame(x.toarray(), columns=v.get_feature_names())\n",
    "df.drop('features', axis=1, inplace=True)\n",
    "df = df.join(df1.set_index(df.index))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unexpected-italic",
   "metadata": {},
   "source": [
    "#### Date Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "featured-yield",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['created'] = pd.to_datetime(df['created'])\n",
    "df['created_year'] = df['created'].dt.year\n",
    "df['created_month'] = df['created'].dt.month\n",
    "df['created_day_of_month'] = df['created'].dt.day\n",
    "df['created_day_of_week'] = df['created'].dt.dayofweek\n",
    "df['created_hour'] = df['created'].dt.hour"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "structured-flavor",
   "metadata": {},
   "source": [
    "#### Image Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "willing-karma",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['num_photos'] = df['photos'].apply(len)\n",
    "df['photos_per_bedroom'] = df[['num_photos','bedrooms']].apply(lambda x: x[0]/x[1] if x[1]!=0 else 0, axis=1)\n",
    "df['photos_per_bathroom'] = df[['num_photos','bathrooms']].apply(lambda x: x[0]/x[1] if x[1]!=0 else 0, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "respective-feeling",
   "metadata": {},
   "source": [
    "#### Numerical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extra-retention",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['total_rooms'] = df['bathrooms'] + df['bedrooms']\n",
    "df['price_per_room'] = df[['price','total_rooms']].apply(lambda x: x[0]/x[1] if x[1] != 0 else 0, axis=1)\n",
    "df['price_per_bedroom'] = df[['price','bedrooms']].apply(lambda x: x[0]/x[1] if x[1] != 0 else 0, axis=1)\n",
    "df['price_per_bathroom'] = df[['price','bathrooms']].apply(lambda x: x[0]/x[1] if x[1] != 0 else 0, axis=1)\n",
    "\n",
    "df['price_per_word_description'] = df[['price','num_words_description']].apply(lambda x: x[0]/x[1] if x[1] != 0 else 0, axis=1)\n",
    "df['price_per_length_description'] = df[['price','length_description']].apply(lambda x: x[0]/x[1] if x[1] != 0 else 0, axis=1)\n",
    "df['price_per_feature'] = df[['price','num_features']].apply(lambda x: x[0]/x[1] if x[1] != 0 else 0, axis=1)\n",
    "df['price_per_photo'] = df[['price','num_photos']].apply(lambda x: x[0]/x[1] if x[1] != 0 else 0, axis=1)\n",
    "\n",
    "central_park_coordinates = (40.7799963,-73.970621)\n",
    "df['distance_to_central_park'] = df[['latitude','longitude']].apply(\n",
    "        lambda x: functions.calculate_distance_between_coordinates(central_park_coordinates,(x[0],x[1])), axis=1)\n",
    "\n",
    "wall_street_coordinates = (40.7059692,-74.0099558)\n",
    "df['distance_to_wall_street'] = df[['latitude','longitude']].apply(\n",
    "        lambda x: functions.calculate_distance_between_coordinates(wall_street_coordinates,(x[0],x[1])), axis=1)\n",
    "\n",
    "times_square_coordinates = (40.7567473,-73.9888876)\n",
    "df['distance_to_times_square'] = df[['latitude','longitude']].apply(\n",
    "        lambda x: functions.calculate_distance_between_coordinates(times_square_coordinates,(x[0],x[1])), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "selective-revision",
   "metadata": {},
   "source": [
    "### Correlation of Features and Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "canadian-checklist",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Object columns dropped\"\"\"\n",
    "df = df.drop(['building_id', 'listing_id', 'description', 'created', 'display_address', 'manager_id', \n",
    "              'photos', 'street_address'], axis=1) \n",
    "\n",
    "# Convert target values into ordinal values \n",
    "\n",
    "df_corr = df.corr()\n",
    "df_corr_abs = np.abs(df_corr['interest_level'])\n",
    "\n",
    "df_corr_abs_sort = df_corr_abs.sort_values(ascending = False)\n",
    "print(df_corr_abs_sort)\n",
    "\n",
    "sns.set(rc={'figure.figsize':(15.7,10.27)})\n",
    "sns.heatmap(df.corr())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abroad-coverage",
   "metadata": {},
   "source": [
    "### Data Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "magnetic-magic",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy = df.drop(\"interest_level\", axis=1)\n",
    "scaler = preprocessing.MinMaxScaler()\n",
    "names = df_copy.columns\n",
    "d = scaler.fit_transform(df_copy)\n",
    "scaled_df = pd.DataFrame(d, columns=names)\n",
    "scaled_df.head()\n",
    "scaled_df= scaled_df.join(df[['interest_level']].set_index(scaled_df.index))\n",
    "scaled_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aggressive-ordering",
   "metadata": {},
   "source": [
    "### Splitting of the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intermediate-explorer",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dev, df_rest = train_test_split(scaled_df, test_size=0.3)\n",
    "df_test, df_val = train_test_split(df_rest, test_size=0.5)\n",
    "\n",
    "X_val =  df_val.drop(\"interest_level\", axis=1)\n",
    "y_val = df_val[\"interest_level\"]\n",
    "\n",
    "X_test = df_test.drop(\"interest_level\", axis=1)\n",
    "y_test = df_test[\"interest_level\"]\n",
    "\n",
    "X_dev = df_dev.drop(\"interest_level\", axis=1)\n",
    "y_dev = df_dev[\"interest_level\"]\n",
    "\n",
    "\n",
    "X = scaled_df.drop(\"interest_level\", axis=1)\n",
    "y = scaled_df[\"interest_level\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "italic-grace",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning  for the Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "built-lodge",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
