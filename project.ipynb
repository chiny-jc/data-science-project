{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "thousand-murder",
   "metadata": {},
   "source": [
    "### Group C\n",
    "- Campos, Joshua\n",
    "- Halili, Gesara\n",
    "- Nwuzor, Chisom\n",
    "- Tran, Quynh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intense-peace",
   "metadata": {},
   "source": [
    "# Data Science Project 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "human-discretion",
   "metadata": {},
   "source": [
    "In this project, we will be tackling a supervised classification problem using the data from the Kaggle competition \"[Two Sigma Connect: Rental Listing Inquiries](https://www.kaggle.com/c/two-sigma-connect-rental-listing-inquiries)\". Our task will be to classify rental listing inquiries, and predict how much interest they will receive according to three categories: Low, Medium and High. \n",
    "\n",
    "For this project, we will be using three different models, and comparing their performance to find the best one for this specific task. The models will be:\n",
    "- Regularized Linear Models\n",
    "- Trees\n",
    "- Random Forests\n",
    "- Neural Networks\n",
    "\n",
    "Our classification problem will be composed of various sections: \n",
    "- Preparation & Exploratory Data Analysis\n",
    "- Data Preprocessing & Feature Engineering\n",
    "- Model Performance & Hyper-Parameter Tuning\n",
    "- Prediction Explanation & Story Telling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intended-alcohol",
   "metadata": {},
   "source": [
    "## Preparation & Exploratory Data Analysis\n",
    "\n",
    "### Importing the libraries\n",
    "\n",
    "The first thing we have to do is to import all the libraries that we will be using for our project, which include the typical libraries for data science, such as Numpy, Pandas, Matplotlib, and Scikit-Learn, among others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "irish-genome",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "otherwise-asbestos",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import string \n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras import optimizers\n",
    "from keras.layers import Dense, Dropout\n",
    "\n",
    "from pylab import rcParams\n",
    "from re import search \n",
    "\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, GridSearchCV, RepeatedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import OneHotEncoder, MultiLabelBinarizer, LabelEncoder, OrdinalEncoder\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from pdpbox import pdp, get_dataset, info_plots\n",
    "from sklearn.inspection import permutation_importance\n",
    "from time import time\n",
    "from rfpimp import *\n",
    "import shap\n",
    "\n",
    "import functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "perfect-verification",
   "metadata": {},
   "source": [
    "### Reading the file\n",
    "\n",
    "Here we read the data, which is stored as a JSON file. We will be only using the training data for the whole project, which we will split into training and testing sets respectively. This is done to decrease the processing power and time required for the model training and the hyper-parameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "judicial-dealing",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/train.json') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continuous-mount",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis\n",
    "\n",
    "Before continuing, we must get a grasp of what the data looks like. For this, we print the head of the data, as well as some statistics and information about the variables. Besides this, we also take a look at the unique number of values for the display addresses, the manager ids, and the features. By doing this, we understand if it would be wise to treat this features as categorical or not. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "increasing-express",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_head = df.head()\n",
    "df_describe = df.describe()\n",
    "df_info = df.info()\n",
    "\n",
    "print('\\nNum. of Unique Display Addresses: {}'.format(df['display_address'].nunique()))\n",
    "print('Num. of Unique Manager IDs: {}'.format(df['manager_id'].nunique()))\n",
    "\n",
    "all_unique_features = set()    \n",
    "df['features'].apply(lambda x: functions.add_unique_elements(x, all_unique_features))\n",
    "print('Num. of Unique Features: {}'.format(len(all_unique_features)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "heated-simon",
   "metadata": {},
   "source": [
    "We also split the variables into some of their different data types for easier handling later on. And finally, we encode the target variable from text to numerical values, ranging from 0 to 2, and plot the counts for each class to get an idea of how balanced or unbalanced the dataset is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "offshore-compatibility",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_vars = ['bathrooms','bedrooms','latitude','longitude','price']\n",
    "cat_vars = ['building_id','manager_id','display_address','street_address']\n",
    "text_vars = ['description','features']\n",
    "\n",
    "rcParams['figure.figsize'] = 11.7,8.27\n",
    "df['interest_level'] = df['interest_level'].apply(lambda x: 0 if x=='low' else 1 if x=='medium' else 2)\n",
    "# Check homogeneity of target values\n",
    "sns.countplot('interest_level', data=df)\n",
    "plt.title('Unbalanced Classes')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dated-strike",
   "metadata": {},
   "source": [
    "## Data Preprocessing & Feature Engineering\n",
    "\n",
    "### Feature Engineering\n",
    "\n",
    "After taking a look at the current features available in the dataset, we notice that there are some variables with which we cannot work just yet. So as a first step, before going into the modeling journey, we decide to do some feature engineering in order to extract some more value from the existing features and create new variables that might help us in the next steps. \n",
    "\n",
    "For this section, we used, as a reference, the notebook available in Kaggle called \"[TwoSigmaRenthop - Advanced Feature Engineering](https://www.kaggle.com/chriscc/twosigmarenthop-advanced-feature-engineering)\". This notebook helped us understand different ways to handle several of the variables present in the notebook to add more value to our project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intelligent-struggle",
   "metadata": {},
   "source": [
    "#### Categorical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fatty-theater",
   "metadata": {},
   "outputs": [],
   "source": [
    "addresses = ['display_address', 'street_address']\n",
    "\n",
    "for address in addresses:\n",
    "    print(address)\n",
    "    ''' delete rows that contain descriptions instead of actual addresses '''\n",
    "    address_delete = [] \n",
    "    for i in range(len(df)):\n",
    "        address_val = df[address][i]\n",
    "        if search('!' or '*', address_val):\n",
    "            address_delete.append(i)\n",
    "\n",
    "    df = df.drop(df.index[address_delete])\n",
    "    print(\"Num. of deleted addresses: {}\".format(\n",
    "        len(address_delete)))\n",
    "    \n",
    "    \n",
    "    ''' Data Cleaning '''\n",
    "    address_column = df[address]\n",
    "    address_column_transformed = ( address_column\n",
    "                                           .apply(str.upper)\n",
    "                                           .apply(lambda x: x.replace('WEST','W'))\n",
    "                                           .apply(lambda x: x.replace('EAST','E'))\n",
    "                                           .apply(lambda x: x.replace('STREET','ST'))\n",
    "                                           .apply(lambda x: x.replace('AVENUE','AVE'))\n",
    "                                           .apply(lambda x: x.replace('BOULEVARD','BLVD'))\n",
    "                                           .apply(lambda x: x.replace('.',''))\n",
    "                                           .apply(lambda x: x.replace(',',''))\n",
    "                                           .apply(lambda x: x.replace('&',''))\n",
    "                                           .apply(lambda x: x.replace('(',''))\n",
    "                                           .apply(lambda x: x.replace(')',''))\n",
    "                                           .apply(lambda x: x.strip())\n",
    "                                           #.apply(lambda x: re.sub('(?<=\\d)[A-Z]{2}', '', x))\n",
    "                                           .apply(lambda x: re.sub('[^A-Za-z0-9]+ ', '', x)) #remove all special characters and punctuaction\n",
    "                                           .apply(lambda x: x.replace('FIRST','1'))\n",
    "                                           .apply(lambda x: x.replace('SECOND','2'))\n",
    "                                           .apply(lambda x: x.replace('THIRD','3'))\n",
    "                                           .apply(lambda x: x.replace('FOURTH','4'))\n",
    "                                           .apply(lambda x: x.replace('FIFTH','5'))\n",
    "                                           .apply(lambda x: x.replace('SIXTH','6'))\n",
    "                                           .apply(lambda x: x.replace('SEVENTH','7'))\n",
    "                                           .apply(lambda x: x.replace('EIGHTH','8'))\n",
    "                                           .apply(lambda x: x.replace('EIGTH','8'))\n",
    "                                           .apply(lambda x: x.replace('NINTH','9'))\n",
    "                                           .apply(lambda x: x.replace('TENTH','10'))\n",
    "                                           .apply(lambda x: x.replace('ELEVENTH','11'))\n",
    "                                         )\n",
    "\n",
    "    print(\"Num. of Unique Addresses after Transformation: {}\".format(\n",
    "        address_column_transformed.nunique()))\n",
    "\n",
    "    df[address] = address_column_transformed \n",
    "\n",
    "#Count features\n",
    "display=df[\"display_address\"].value_counts()\n",
    "manager_id=df[\"manager_id\"].value_counts()\n",
    "building_id=df[\"building_id\"].value_counts()\n",
    "street=df[\"street_address\"].value_counts()\n",
    "\n",
    "df[\"display_count\"]=df[\"display_address\"].apply(lambda x:display[x])\n",
    "df[\"manager_count\"]=df[\"manager_id\"].apply(lambda x:manager_id[x])  \n",
    "df[\"building_count\"]=df[\"building_id\"].apply(lambda x:building_id[x])\n",
    "df[\"street_count\"]=df[\"street_address\"].apply(lambda x:street[x])\n",
    "\n",
    "#Numeric-categorical interactions (price and building)\n",
    "price_by_building = df.groupby('building_id')['price'].agg([np.min,np.max,np.mean]).reset_index()\n",
    "price_by_building.columns = ['building_id','min_price_by_building',\n",
    "                            'max_price_by_building','mean_price_by_building']\n",
    "df = pd.merge(df,price_by_building, how='left',on='building_id')\n",
    "df = df.drop(df.index[address_delete])\n",
    "\n",
    "OE = OrdinalEncoder()\n",
    "for cat_var in cat_vars:\n",
    "    print (\"Ordinal Encoding %s\" % (cat_var))\n",
    "    df[cat_var]=OE.fit_transform(df[[cat_var]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arabic-catholic",
   "metadata": {},
   "source": [
    "#### Text Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mexican-soundtrack",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Count number of special characters in description:\n",
    "Studies have shown that titles with excessive all caps and special characters give renters the impression \n",
    "that the listing is fraudulent â€“ i.e. BEAUTIFUL***APARTMENT***CHELSEA.\n",
    "'''\n",
    "\n",
    "df['num_of_#']=df.description.apply(lambda x:x.count('#'))\n",
    "df['num_of_!']=df.description.apply(lambda x:x.count('!'))\n",
    "df['num_of_$']=df.description.apply(lambda x:x.count('$'))\n",
    "df['num_of_*']=df.description.apply(lambda x:x.count('*'))\n",
    "df['num_of_>']=df.description.apply(lambda x:x.count('>'))\n",
    "\n",
    "#Derive new features (check for phone and email) out of description \n",
    "df['has_phone'] = df['description'].apply(lambda x:re.sub('['+string.punctuation+']', '', x).split())\\\n",
    "        .apply(lambda x: [s for s in x if s.isdigit()])\\\n",
    "        .apply(lambda x: len([s for s in x if len(str(s))==10]))\\\n",
    "        .apply(lambda x: 1 if x>0 else 0)\n",
    "\n",
    "print('Has phone: ' + str(np.round(len(np.where(df['has_phone'] == 1)[0]) / len(df), 2)) + '%')\n",
    "print('Has no phone: '+ str(np.round(len(np.where(df['has_phone'] == 0)[0]) / len(df), 2)) + '%')\n",
    "\n",
    "df['has_email'] = df['description'].apply(lambda x: 1 if '@renthop.com' in x else 0)\n",
    "print('Has email: '+ str(np.round(len(np.where(df['has_email'] == 1)[0]) / len(df), 2))+ '%')\n",
    "print('Has no email: '+ str(np.round(len(np.where(df['has_email'] == 0)[0]) / len(df), 2))+ '%')\n",
    "\n",
    "# Count length, num of words and features of description of features \n",
    "display_address_column = df['description']\n",
    "df['description'] = [functions.text_cleaner(x) for x in display_address_column]\n",
    "\n",
    "df['length_description'] = df['description'].apply(lambda x: len(x))\n",
    "df['num_words_description'] = df['description'].apply(lambda x: len(x.split(\" \")))\n",
    "\n",
    "df['num_features'] = df['features'].apply(len)\n",
    "\n",
    "#Extract tokens of features and OHE \n",
    "v = CountVectorizer(stop_words='english', max_features=50)\n",
    "x = v.fit_transform(df['features']\\\n",
    "                                     .apply(lambda x: \" \".join([\"_\".join(i.split(\" \")) for i in x])))\n",
    "\n",
    "df1 = pd.DataFrame(x.toarray(), columns=v.get_feature_names())\n",
    "df.drop('features', axis=1, inplace=True)\n",
    "df = df.join(df1.set_index(df.index))\n",
    "\n",
    "tokens_features = v.get_feature_names()\n",
    "print(tokens_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sticky-novel",
   "metadata": {},
   "source": [
    "#### Date Variables\n",
    "\n",
    "In the dataset, we have one text variable that refers to the date the rental advertisement was published. In order to use this variable we have to convert it from text to actual datetime type. After we do this, we decide to get some more features from this variable, including the year, month, day of month, day of week, and hour of publication. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "binding-albert",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['created'] = pd.to_datetime(df['created'])\n",
    "df['created_year'] = df['created'].dt.year\n",
    "df['created_month'] = df['created'].dt.month\n",
    "df['created_day_of_month'] = df['created'].dt.day\n",
    "df['created_day_of_week'] = df['created'].dt.dayofweek\n",
    "df['created_hour'] = df['created'].dt.hour"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "documentary-debate",
   "metadata": {},
   "source": [
    "#### Image Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "institutional-curtis",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['num_photos'] = df['photos'].apply(len)\n",
    "df['photos_per_bedroom'] = df[['num_photos','bedrooms']].apply(lambda x: x[0]/x[1] if x[1]!=0 else 0, axis=1)\n",
    "df['photos_per_bathroom'] = df[['num_photos','bathrooms']].apply(lambda x: x[0]/x[1] if x[1]!=0 else 0, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coordinate-qatar",
   "metadata": {},
   "source": [
    "#### Numerical Variables\n",
    "\n",
    "From the previous section, we noticed that we have several numerical variables, and after many of the transformations, we end up with even more of them. Becase of this, we decide to create some logical interactions that might have an impact on our model. \n",
    "\n",
    "We decide to calculate the total numer of rooms, which is the bedrooms plus the bathrooms. We then decide to calculate the price per room, bedroom and bathroom, where we divide the price by the number of the previously mentioned values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "local-jenny",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['total_rooms'] = df['bathrooms'] + df['bedrooms']\n",
    "df['price_per_room'] = df[['price','total_rooms']].apply(lambda x: x[0]/x[1] if x[1] != 0 else 0, axis=1)\n",
    "df['price_per_bedroom'] = df[['price','bedrooms']].apply(lambda x: x[0]/x[1] if x[1] != 0 else 0, axis=1)\n",
    "df['price_per_bathroom'] = df[['price','bathrooms']].apply(lambda x: x[0]/x[1] if x[1] != 0 else 0, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surprising-staff",
   "metadata": {},
   "source": [
    "We also decide to do something similar like before, but this time we calculate the price per word in the word description, as well as the price per length of the description. Moreover, we do the same thing with the features and photos, by calculating the price by each of them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conceptual-address",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['price_per_word_description'] = df[['price','num_words_description']].apply(lambda x: x[0]/x[1] if x[1] != 0 else 0, axis=1)\n",
    "df['price_per_length_description'] = df[['price','length_description']].apply(lambda x: x[0]/x[1] if x[1] != 0 else 0, axis=1)\n",
    "df['price_per_feature'] = df[['price','num_features']].apply(lambda x: x[0]/x[1] if x[1] != 0 else 0, axis=1)\n",
    "df['price_per_photo'] = df[['price','num_photos']].apply(lambda x: x[0]/x[1] if x[1] != 0 else 0, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intelligent-omaha",
   "metadata": {},
   "source": [
    "Finally, we also notice that we have the location of the rented properties. We know that the location usually has an impact on the real estate, so we decide to calculate the distances to some well-known areas of New York City, including the Central Park, Wall Street, and Times Square. In order to calculate the distances, we use a function that we previously defined, that takes the differences of the coordinates and converts the difference into kilometers, so it makes more sense to us. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rural-injury",
   "metadata": {},
   "outputs": [],
   "source": [
    "central_park_coordinates = (40.7799963,-73.970621)\n",
    "df['distance_to_central_park'] = df[['latitude','longitude']].apply(\n",
    "        lambda x: functions.calculate_distance_between_coordinates(central_park_coordinates,(x[0],x[1])), axis=1)\n",
    "\n",
    "wall_street_coordinates = (40.7059692,-74.0099558)\n",
    "df['distance_to_wall_street'] = df[['latitude','longitude']].apply(\n",
    "        lambda x: functions.calculate_distance_between_coordinates(wall_street_coordinates,(x[0],x[1])), axis=1)\n",
    "\n",
    "times_square_coordinates = (40.7567473,-73.9888876)\n",
    "df['distance_to_times_square'] = df[['latitude','longitude']].apply(\n",
    "        lambda x: functions.calculate_distance_between_coordinates(times_square_coordinates,(x[0],x[1])), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "medieval-rapid",
   "metadata": {},
   "source": [
    "### Correlation of Features and Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nasty-flight",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Object columns dropped\"\"\"\n",
    "df = df.drop(['building_id', 'listing_id', 'description', 'created', 'created_year','display_address', 'manager_id', \n",
    "              'photos', 'street_address'], axis=1) \n",
    "\n",
    "# Convert target values into ordinal values \n",
    "\n",
    "df_corr = df.corr()\n",
    "df_corr_abs = np.abs(df_corr['interest_level'])\n",
    "\n",
    "df_corr_abs_sort = df_corr_abs.sort_values(ascending = False)\n",
    "print(df_corr_abs_sort)\n",
    "\n",
    "sns.set(rc={'figure.figsize':(15.7,10.27)})\n",
    "sns.heatmap(df.corr())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "romantic-tonight",
   "metadata": {},
   "source": [
    "## Model Performance & Hyper-Parameter Tuning\n",
    "\n",
    "### Splitting of the Dataset\n",
    "\n",
    "After having performed the feature engineering and decided that our dataset is ready to be modeled, we first have to split our dataset into training and testing set. For this case, we first separate our independent variables (X) from our target variable (y). Since we are going to be using cross-validation, we decide to split only into training and testing set, without manually splitting also into a validation set, which will be created automatically during the cross-validation. As a testing size, we decide to use 15% of the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extreme-relations",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(\"interest_level\", axis=1)\n",
    "y = df[\"interest_level\"]\n",
    "\n",
    "X_train, X_test, y_train , y_test = train_test_split(X, y, test_size=0.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spectacular-adams",
   "metadata": {},
   "source": [
    "### Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "administrative-builder",
   "metadata": {},
   "outputs": [],
   "source": [
    "sel = SelectFromModel(RandomForestClassifier())\n",
    "sel.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "#To see which feature are important\n",
    "sel.get_support()\n",
    "\n",
    "\n",
    "#to get the number of important features\n",
    "selected_feat= X_train.columns[(sel.get_support())]\n",
    "len(selected_feat)\n",
    "\n",
    "\n",
    "#to get the name of feature selected\n",
    "print(selected_feat)\n",
    "\n",
    "\n",
    "X_train= X_train[selected_feat]\n",
    "X_test= X_test[selected_feat]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pregnant-klein",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning  for the Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comic-punch",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Number of trees in random forest\n",
    "n_estimators = np.arange(start=100, stop=2001, step=10)\n",
    " #Number of features to consider at every split'''\n",
    "max_features = ['auto', 'sqrt']\n",
    " #Maximum number of levels in tree'''\n",
    "max_depth = np.arange(start=10, stop=111, step=5)\n",
    "#max_depth.append(None)\n",
    " #Minimum number of samples required to split a node'''\n",
    "min_samples_split = np.arange(start=2, stop=101)\n",
    " #Minimum number of samples required at each leaf node'''\n",
    "min_samples_leaf = np.arange(start=2, stop=101)\n",
    "#Method of selecting samples for training each tree'''\n",
    "bootstrap = [True, False]\n",
    "\n",
    "\n",
    "'''Create the random grid'''\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "\n",
    "\n",
    "'''First create the base model to tune'''\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "''' Use the random grid to search for best hyperparameters'''\n",
    "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 20, cv = 5, verbose=2, \n",
    "                               random_state=123, n_jobs = -1)\n",
    "\n",
    "'''Fit the random search model'''\n",
    "rf_random.fit(X_train, y_train)\n",
    "\n",
    "print('test')\n",
    "\n",
    "best_params = rf_random.best_params_\n",
    "\n",
    "print(best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hairy-roommate",
   "metadata": {},
   "source": [
    "### Pruning of the Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chief-relief",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = DecisionTreeClassifier(criterion = \"gini\", random_state = 123) \n",
    "tree = tree.fit(X_train, y_train) \n",
    "y_pred = tree.predict(X_test) \n",
    "print (\"Accuracy : \", accuracy_score(y_test,y_pred)*100) \n",
    "\n",
    "\n",
    "\n",
    "#ploting the tree\n",
    "\n",
    "from sklearn.tree import plot_tree\n",
    "plt.figure(figsize = (15,17))\n",
    "plot_tree(tree, filled = True, rounded = True, class_names = ['Low Interest', 'Medium Interest', 'High Interest'], feature_names = X.columns)[0]\n",
    "\n",
    "#determine values for alpha\n",
    "path = tree.cost_complexity_pruning_path(X_train, y_train)\n",
    "\n",
    "#extract different values for alpha\n",
    "ccp_alphas = path.ccp_alphas \n",
    "ccp_alphas = ccp_alphas[:-1]\n",
    "\n",
    "\n",
    "#create an array to store the resuls of each fold during cross validation\n",
    "alpha_loop_values = []\n",
    "\n",
    "# For each candidate value alpha, we will run 5-fold cross validation\n",
    "# Then we will store the mean and std of the scores(accuracy) for each call\n",
    "# to cross_val_score in alpha_loop_values...\n",
    "\n",
    "for ccp_alpha in ccp_alphas:\n",
    "    tree = DecisionTreeClassifier(random_state=0, ccp_alpha=ccp_alpha)\n",
    "    scores = cross_val_score(tree, X_train, y_train, cv=5)\n",
    "    alpha_loop_values.append([ccp_alpha, np.mean(scores), np.std(scores)])\n",
    "#Now we can draw a graph of the means and std of the scores\n",
    "#for each candidate value for alpha\n",
    "alpha_results = pd.DataFrame(alpha_loop_values, \n",
    "                             columns=['alpha', 'mean_accuracy', 'std'])\n",
    "\n",
    "alpha_results.plot(x='alpha',\n",
    "                   y='mean_accuracy',\n",
    "                   yerr = 'std',\n",
    "                   marker = 'o',\n",
    "                  linestyle ='--')\n",
    "\n",
    "\n",
    "#Printing the best alpha\n",
    "\n",
    "column = alpha_results[\"mean_accuracy\"]\n",
    "max_index = column.idxmax()\n",
    "max_index\n",
    "\n",
    "\n",
    "ideal_ccp_alpha = alpha_results.iloc[max_index]['alpha']\n",
    "#ideal_ccp_alpha \n",
    "#Prunning the classification tree\n",
    "\n",
    "tree_pruned = DecisionTreeClassifier(random_state = 123,\n",
    "                                    ccp_alpha=ideal_ccp_alpha)\n",
    "tree_pruned = tree_pruned.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "y_pred = tree_pruned.predict(X_test) \n",
    "print (\"Accuracy : \", accuracy_score(y_test,y_pred)*100) \n",
    "\n",
    "#plotting the best tree\n",
    "plt.figure(figsize = (20,17))\n",
    "plot_tree(tree_pruned, filled = True, rounded = True,class_names = ['Low Interest', 'Medium Interest', 'High Interest'], feature_names = X.columns)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "general-principle",
   "metadata": {},
   "source": [
    "### Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collectible-landscape",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining model parameters from the tuned parameter\n",
    "model_params = {\n",
    "    'random_forest': {\n",
    "        'model': RandomForestClassifier(),\n",
    "        'params': {\n",
    "            'n_estimators': best_params['n_estimators'],\n",
    "            'min_samples_split': best_params['min_samples_split'],\n",
    "            'min_samples_leaf': best_params['min_samples_leaf'],\n",
    "            'max_features': best_params['max_features'],\n",
    "            'max_depth': best_params['max_depth'],\n",
    "            'bootstrap': best_params['bootstrap']\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    'decision_tree': {\n",
    "        'model': DecisionTreeClassifier(),\n",
    "        'params': {\n",
    "            'ccp_alpha': ideal_ccp_alpha\n",
    "        }\n",
    "    },\n",
    "    \n",
    "}\n",
    "\n",
    "#Loop over both decision tree and random forest to determine the best model for the given dataset\n",
    "scores = []\n",
    "print(model_params.items())\n",
    "\n",
    "trained_models = []\n",
    "for model_name, mp in model_params.items():\n",
    "    clf = mp['model']\n",
    "    clf.set_params(**mp['params'])\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict_proba(X_test)\n",
    "    scores.append({\n",
    "        'model': model_name,\n",
    "        'train_score': clf.score(X_train, y_train),\n",
    "        'test_score': clf.score(X_test, y_test),\n",
    "        'log_loss': log_loss(y_test, y_pred)\n",
    "    })\n",
    "    trained_models.append(clf)\n",
    "    \n",
    "best_model = pd.DataFrame(scores, columns= ['model','train_score','test_score', 'log_loss'])\n",
    "print(best_model)\n",
    "\n",
    "rf = trained_models[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hollow-malawi",
   "metadata": {},
   "source": [
    "## Prediction Explanation & Story Telling\n",
    "\n",
    "### Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "described-plain",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MDI\n",
    "MDI_importances = rf.feature_importances_\n",
    "indices = np.argsort(MDI_importances)\n",
    "features = X_train.columns\n",
    "\n",
    "#MDA\n",
    "MDA_test = permutation_importance(rf, X_test, y_test, n_repeats=10, random_state=123)\n",
    "sorted_idx1 = MDA_test.importances_mean.argsort()\n",
    "MDA_train = permutation_importance(rf, X_train, y_train, n_repeats=10, random_state=123)\n",
    "sorted_idx2 = MDA_train.importances_mean.argsort()\n",
    "\n",
    "#Plotting\n",
    "plt.rcParams[\"figure.figsize\"]=15,5\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3)\n",
    "ax1.set_title('MDI Importances')\n",
    "ax1.barh(range(len(indices)), MDI_importances[indices], color='b', align='center')\n",
    "ax1.set_yticks( np.arange(len(X_train.columns)))\n",
    "ax1.set_yticklabels(features[indices])\n",
    "ax1.set(xlabel='Relative Importance')\n",
    "ax2.boxplot(MDA_train.importances[sorted_idx2].T,\n",
    "vert=False, labels=features[sorted_idx2])\n",
    "tpmp=ax2.set_title(\"Permutation Importances (train)\")\n",
    "ax3.boxplot(MDA_test.importances[sorted_idx1].T,\n",
    "vert=False, labels=features[sorted_idx1])\n",
    "tpmp=ax3.set_title(\"Permutation Importances (test)\")\n",
    "fig.tight_layout()\n",
    "\n",
    "#Table of results\n",
    "# record times for comparison\n",
    "t0 = time()\n",
    "# Gini importance\n",
    "gini_imp = pd.DataFrame({'Feature': X_train.columns, 'Gini Importance': rf.feature_importances_}).set_index('Feature')\n",
    "\n",
    "t1 = time()# Permutation importance for train\n",
    "perm_imp = importances(rf, X_train, y_train)\n",
    "\n",
    "t2 = time()\n",
    "res= gini_imp.merge(perm_imp, left_index=True, right_index=True).reset_index().\\\n",
    "        rename(columns={'Importance': 'Permutation Importance'})\n",
    "res.loc[len(X_train.columns)+1] = ['runtime(s)', t1-t0, t2-t1]\n",
    "\n",
    "print(res)\n",
    "\n",
    "top5_feat = res[:-1].sort_values(by=['Permutation Importance'], ascending=False).head(5)\n",
    "print('Top 5 features with higher importance:', top5_feat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "artistic-bermuda",
   "metadata": {},
   "source": [
    "Explain what pdp-s are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "capable-physiology",
   "metadata": {},
   "outputs": [],
   "source": [
    "ploting_pdp(f=top5_feat.iloc[0,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mathematical-rebate",
   "metadata": {},
   "source": [
    "explain result 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "corrected-binary",
   "metadata": {},
   "outputs": [],
   "source": [
    "ploting_pdp(f=top5_feat.iloc[1,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hourly-passing",
   "metadata": {},
   "source": [
    "explain result 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "protective-registrar",
   "metadata": {},
   "outputs": [],
   "source": [
    "ploting_pdp(f=top5_feat.iloc[2,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "strong-panic",
   "metadata": {},
   "source": [
    "explain result 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "joined-alcohol",
   "metadata": {},
   "outputs": [],
   "source": [
    "ploting_pdp(f=top5_feat.iloc[3,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adopted-veteran",
   "metadata": {},
   "source": [
    "explain result 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "controlled-receipt",
   "metadata": {},
   "outputs": [],
   "source": [
    "ploting_pdp(f=top5_feat.iloc[4,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "synthetic-employer",
   "metadata": {},
   "source": [
    "explain result 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jewish-german",
   "metadata": {},
   "source": [
    "Explain nex steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intelligent-retailer",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_top_abs_correlations(df, n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aggressive-eight",
   "metadata": {},
   "source": [
    "Explain 2 dimension pdp-s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "satisfactory-volleyball",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot 2 dimension pdp for top 5 correlations\n",
    "features_to_plot = ['price', 'bedrooms'] #manually add the top 5 features\n",
    "two_dim_pdp(f=features_to_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "flush-joint",
   "metadata": {},
   "source": [
    "Exlain result 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aware-headset",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot 2 dimension pdp for top 5 correlations\n",
    "features_to_plot = ['price', 'bedrooms']\n",
    "two_dim_pdp(f=features_to_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hindu-fossil",
   "metadata": {},
   "source": [
    "Exlain result 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "golden-nursery",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot 2 dimension pdp for top 5 correlations\n",
    "features_to_plot = ['price', 'bedrooms']\n",
    "two_dim_pdp(f=features_to_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "drawn-assurance",
   "metadata": {},
   "source": [
    "Exlain result 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "shared-characteristic",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot 2 dimension pdp for top 5 correlations\n",
    "features_to_plot = ['price', 'bedrooms']\n",
    "two_dim_pdp(f=features_to_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "colored-bundle",
   "metadata": {},
   "source": [
    "Exlain result 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "specified-bradley",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot 2 dimension pdp for top 5 correlations\n",
    "features_to_plot = ['price', 'bedrooms']\n",
    "two_dim_pdp(f=features_to_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blind-profession",
   "metadata": {},
   "source": [
    "Exlain result 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "union-french",
   "metadata": {},
   "source": [
    "Explain what shap values are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "illegal-depression",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_summary(rf, X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "urban-bermuda",
   "metadata": {},
   "source": [
    "Explain results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "demanding-killer",
   "metadata": {},
   "source": [
    "Explain what dependency contribution is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cloudy-freight",
   "metadata": {},
   "outputs": [],
   "source": [
    "dependency_contribution(f=top5_feat.iloc[0,0], i=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "annoying-emission",
   "metadata": {},
   "source": [
    "Explain result 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "approved-image",
   "metadata": {},
   "outputs": [],
   "source": [
    "dependency_contribution(f=top5_feat.iloc[0,0], i=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "disciplinary-sheep",
   "metadata": {},
   "source": [
    "Explain result 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lasting-wallpaper",
   "metadata": {},
   "outputs": [],
   "source": [
    "dependency_contribution(f=top5_feat.iloc[0,0], i=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "finnish-muslim",
   "metadata": {},
   "source": [
    "Explain result 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sharing-mexican",
   "metadata": {},
   "outputs": [],
   "source": [
    "dependency_contribution(f=top5_feat.iloc[1,0], i=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "crude-outdoors",
   "metadata": {},
   "source": [
    "Explain result 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "developing-shell",
   "metadata": {},
   "outputs": [],
   "source": [
    "dependency_contribution(f=top5_feat.iloc[1,0], i=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "optimum-hypothetical",
   "metadata": {},
   "source": [
    "Explain result 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "desperate-margin",
   "metadata": {},
   "outputs": [],
   "source": [
    "dependency_contribution(f=top5_feat.iloc[1,0], i=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amended-swedish",
   "metadata": {},
   "source": [
    "Explain result 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "valid-matter",
   "metadata": {},
   "source": [
    "Explain what forceplots are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "liquid-florida",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = np.random.randint(1,df.shape[0]+1)\n",
    "m = np.random.randint(0,3)\n",
    "forceplots(n, m)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continued-notification",
   "metadata": {},
   "source": [
    "Explain the results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "military-preference",
   "metadata": {},
   "source": [
    "## Extras\n",
    "\n",
    "### Neural Network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "after-stable",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = len(np.unique(y))\n",
    "n_features = X_train.shape[1]\n",
    "\n",
    "ffnn = Sequential()\n",
    "ffnn.add(Dense(256, activation='relu', input_shape=(n_features,)))\n",
    "ffnn.add(Dropout(0.3))\n",
    "ffnn.add(Dense(64, activation='relu'))\n",
    "ffnn.add(Dense(32, activation='relu'))\n",
    "ffnn.add(Dense(n_classes, activation='softmax'))\n",
    "\n",
    "ffnn.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer=optimizers.Adam(lr=0.0001),\n",
    "              metrics=['accuracy'])\n",
    "ffnn.summary()\n",
    "\n",
    "trained_ffnn = ffnn.fit(X_train, y_train,\n",
    "          validation_data = (X_test, y_test),\n",
    "          epochs=50, \n",
    "          batch_size=8, \n",
    "          verbose=1\n",
    "         )\n",
    "\n",
    "rcParams['figure.figsize'] = 10, 5\n",
    "plt.plot(ffnn.history.history['loss'], label = \"Train Loss\")\n",
    "plt.plot(ffnn.history.history['val_loss'], label = \"Test Loss\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.legend()\n",
    "plt.plot()\n",
    "\n",
    "rcParams['figure.figsize'] = 10, 5\n",
    "plt.plot(ffnn.history.history['accuracy'], label = \"Train Accuracy\")\n",
    "plt.plot(ffnn.history.history['val_accuracy'], label = \"Test Accuracy\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.legend()\n",
    "plt.plot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
