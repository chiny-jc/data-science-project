{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group C\n",
    "- Campos, Joshua\n",
    "- Halili, Gesara\n",
    "- Nwuzor, Chisom\n",
    "- Tran, Quynh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Science Project 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, we will be tackling a supervised classification problem using the data from the Kaggle competition \"[Two Sigma Connect: Rental Listing Inquiries](https://www.kaggle.com/c/two-sigma-connect-rental-listing-inquiries)\". Our task will be to classify rental listing inquiries, and predict how much interest they will receive according to three categories: Low, Medium and High. \n",
    "\n",
    "For this project, we will be using three different models, and comparing their performance to find the best one for this specific task. The models will be:\n",
    "- Regularized Linear Models\n",
    "- Trees\n",
    "- Random Forests\n",
    "- Neural Networks\n",
    "\n",
    "Our classification problem will be composed of various sections: \n",
    "- Preparation & Exploratory Data Analysis\n",
    "- Data Preprocessing & Feature Engineering\n",
    "- Model Performance & Hyper-Parameter Tuning\n",
    "- Prediction Explanation & Story Telling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation & Exploratory Data Analysis\n",
    "\n",
    "### Importing the libraries\n",
    "\n",
    "The first thing we have to do is to import all the libraries that we will be using for our project, which include the typical libraries for data science, such as Numpy, Pandas, Matplotlib, and Scikit-Learn, among others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import string \n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras import optimizers\n",
    "from keras.layers import Dense, Dropout\n",
    "\n",
    "from pylab import rcParams\n",
    "from re import search \n",
    "\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, GridSearchCV, RepeatedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import OneHotEncoder, MultiLabelBinarizer, LabelEncoder, OrdinalEncoder\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from pdpbox import pdp, get_dataset, info_plots\n",
    "from sklearn.inspection import permutation_importance\n",
    "from time import time\n",
    "from rfpimp import *\n",
    "import shap\n",
    "\n",
    "import functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the file\n",
    "\n",
    "Here we read the data, which is stored as a JSON file. We will be only using the training data for the whole project, which we will split into training and testing sets respectively. This is done to decrease the processing power and time required for the model training and the hyper-parameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/train.json') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis\n",
    "\n",
    "Before continuing, we must get a grasp of what the data looks like. For this, we print the head of the data, as well as some statistics and information about the variables. Besides this, we also take a look at the unique number of values for the display addresses, the manager ids, and the features. By doing this, we understand if it would be wise to treat this features as categorical or not. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 49352 entries, 4 to 124009\n",
      "Data columns (total 15 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   bathrooms        49352 non-null  float64\n",
      " 1   bedrooms         49352 non-null  int64  \n",
      " 2   building_id      49352 non-null  object \n",
      " 3   created          49352 non-null  object \n",
      " 4   description      49352 non-null  object \n",
      " 5   display_address  49352 non-null  object \n",
      " 6   features         49352 non-null  object \n",
      " 7   latitude         49352 non-null  float64\n",
      " 8   listing_id       49352 non-null  int64  \n",
      " 9   longitude        49352 non-null  float64\n",
      " 10  manager_id       49352 non-null  object \n",
      " 11  photos           49352 non-null  object \n",
      " 12  price            49352 non-null  int64  \n",
      " 13  street_address   49352 non-null  object \n",
      " 14  interest_level   49352 non-null  object \n",
      "dtypes: float64(3), int64(3), object(9)\n",
      "memory usage: 6.0+ MB\n",
      "\n",
      "Num. of Unique Display Addresses: 8826\n",
      "Num. of Unique Manager IDs: 3481\n",
      "Num. of Unique Features: 1556\n"
     ]
    }
   ],
   "source": [
    "df_head = df.head()\n",
    "df_describe = df.describe()\n",
    "df_info = df.info()\n",
    "\n",
    "print('\\nNum. of Unique Display Addresses: {}'.format(df['display_address'].nunique()))\n",
    "print('Num. of Unique Manager IDs: {}'.format(df['manager_id'].nunique()))\n",
    "\n",
    "all_unique_features = set()    \n",
    "df['features'].apply(lambda x: functions.add_unique_elements(x, all_unique_features))\n",
    "print('Num. of Unique Features: {}'.format(len(all_unique_features)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also split the variables into some of their different data types for easier handling later on. And finally, we encode the target variable from text to numerical values, ranging from 0 to 2, and plot the counts for each class to get an idea of how balanced or unbalanced the dataset is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/quynhtran/opt/anaconda3/lib/python3.8/site-packages/seaborn/_decorators.py:36: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAtAAAAH/CAYAAAB+ReriAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAkPElEQVR4nO3df9SmdX0f+PdHBhWrIMJocIYVIzQNkIrLlGBsu0ZsJJ60YBfNuI3SlC0eD7ax9dhq0iYmKXt0E8NRV+3B1QDGBJCYlTWahKDRGglkcIn8CnUSjYyMMAoipgvN4Gf/uK9nvRmeGZ6vzD3PM8zrdc517uv+XN/vdX+vZ84z857v+V7XXd0dAABgZR632gMAAID9iQANAAADBGgAABggQAMAwAABGgAABgjQAAAwQIAGWICquqiq/tP32PctVfUbe3tMj0ZVfbmqXvw99n1hVW3b22MCWC0CNMAyqqqr6thdamsu2K4VVXVKVX28qr5ZVXdX1XVV9dOrPS6ARRCgAXhUqur5ST6Z5NNJjk1yRJLXJvnx1RwXwKII0ADfg6VlCVX1hqq6q6q2LzPjemRVXVVV91XVp6vqWXP931FVt1fVt6rq+qr6B3v4rA9X1deq6t6q+kxVnTB37KKqendV/e70OddW1XPmjp8wjeHuqrqzqn52qj+uqt5UVX9RVd+oqsur6mlz/V5VVX81Hfu5R/hx/EqSi7v7bd399Z65vrtfsZvrWfrc+6rqlqp62dyxY6ef1b1V9fWqumyqV1VdMP2s762qL1TVidOxJ1TVr1bVV6Zr/M9Vdch07Miq+tjczPh/qSr/9gGPir9EAL5335fksCQbkpyT5N1Vdfjc8X+W5JeTHJnkhiQfmjv2p0lOSvK0JL+Z5MNV9cTdfM4nkhyX5OlJPr/LeZLklUl+McnhSbYmOT9JquopSf4wye8leWZms8NXT33+dZIzk/xP07F7krx76nd8kvcmedV07IgkG5cbWFU9Kcnzk1yxm7Ev5y+S/IPMfna/mOQ3quqo6dgvJ/mD6Vo2JnnXVP+xJP8wyd9O8tQkP5nkG9Oxt031k6Zr3JDk56djb0iyLcn6JM9I8rNJemCsAA8jQAN87/4myS91999098eTfDvJD8wd/93u/kx3P5Dk55I8v6qOTpLu/o3u/kZ37+zutyd5wi59/3/d/YHuvm86z1uSPLeqDptr8pHuvq67d2YWrk+a6j+R5Gvd/fbuvn86x7XTsdck+bnu3jZ33rOqal2Ss5J8bG7s/zHJd3bzMzg8s39Ltq/oJza7ng939x3d/Z3uvizJF5OcMh3+myTPSvLMacyfnas/JcnfSVLdfWt3b6+qSvIvk/yb7r67u+9L8r8l2TzX76gkz5r+nP5LdwvQwKMiQAMs78EkB+9SOzizQLbkG1NoXfLfkjx57v3tSzvd/e0kd2c2o5tp6cet03KEb2Y2G3vkroOoqoOq6q3TkodvJfnydGi+7dd2M4ajM5vtXc6zkvzOtLThm0luna75GdMY58f+1/nubO+u7sksXB+1m+MPU1Wvrqob5j77xLnr+XdJKsl1VXVzVf2LaQyfTPJ/ZDZLfmdVXVhVh2Y2s/ykJNfPne/3pnoyW16yNckfVNVfVtWbVjpOgN0RoAGW95Ukx+xSe3aSvxo4x9FLO1X15MyWa9wxrXf+90lekeTw7n5qknszC467+l+SnJHkxZmF7KUxLdd2V7cnec4ejv14dz91bntid381s9nk+bE/KbNlHA/T3f8tyTVJ/ucVjCfTOvD3JXldkiOma79p6Xq6+2vd/S+7+5mZzZK/Z+lpKN39zu4+OckJmS3ZeGOSryf5f5OcMHcdh3X3k6c+93X3G7r7+5P84yT/tqpOW8lYAXZHgAZY3mVJ/kNVbZxuuHtxZgFsZK3vS6vq71fV4zNb23ttd9+e2VKEnUl2JFlXVT+f5NDdnOMpSR7IbAb4SZktT1ipjyX5vqp6/XSj3VOq6oenY/85yflLNzZW1fqqOmM6dkWSn5gb+y9lz/9e/Lsk/7yq3lhVR0zne25VXbpM27+V2RrkHVO7n85sBjrT+5dX1dJ663umtg9W1d+rqh+uqoOT/HWS+5M82N3fySyQX1BVT5/OsaGqXjLt/8R0Y2Il+VZms+wPruzHB7A8ARpgeb+U5HNJPptZkPvfk/yz7r5p4By/meQXMlu6cXJmNxUmye9ndmPgf81sRvv+zC2Z2MUlU5uvJrklyZ+s9MOn9cD/KLPg/7XM1hr/6HT4HUmuzGxpw33TeX946ndzkvOm8W/P7Pp3+0Uo3f25JC+atr+sqruTXJjk48u0vSXJ2zObtb4zyQ8l+eO5Jn8vybVV9e1pfD/T3V/K7D8Y75vG8leZ/YfiV6c+/z6zZRp/Mi1z+cN8dz35cdP7b0+f+Z7u/qPd/9QAHlm5lwIAAFbODDQAAAwQoAEAYIAADQAAAwRoAAAYsG61B7CvHXnkkX3MMces9jAAAFjDrr/++q939/rljh1wAfqYY47Jli1bVnsYAACsYVW12y/OsoQDAAAGCNAAADBAgAYAgAECNAAADBCgAQBggAANAAADBGgAABggQAMAwICFBeiqemJVXVdVf1ZVN1fVL071t1TVV6vqhml76VyfN1fV1qq6rapeMlc/uapunI69s6pqqj+hqi6b6tdW1TGLuh4AAEgWOwP9QJIXdfdzk5yU5PSqOnU6dkF3nzRtH0+Sqjo+yeYkJyQ5Pcl7quqgqf17k5yb5LhpO32qn5Pknu4+NskFSd62wOsBAIDFBeie+fb09uBp6z10OSPJpd39QHd/KcnWJKdU1VFJDu3ua7q7k1yS5My5PhdP+1ckOW1pdhoAABZhoWugq+qgqrohyV1Jrurua6dDr6uqL1TVB6rq8Km2Icntc923TbUN0/6u9Yf06e6dSe5NcsQy4zi3qrZU1ZYdO3bsnYsDAOCAtNAA3d0PdvdJSTZmNpt8YmbLMZ6T2bKO7UnePjVfbua491DfU59dx3Fhd2/q7k3r168fugYAAJi3T57C0d3fTPJHSU7v7junYP2dJO9LcsrUbFuSo+e6bUxyx1TfuEz9IX2qal2Sw5LcvZirAACAxT6FY31VPXXaPyTJi5P8+bSmecnLktw07V+ZZPP0ZI1nZ3az4HXdvT3JfVV16rS++dVJPjrX5+xp/6wkn5zWSQMAwEKsW+C5j0py8fQkjccluby7P1ZVH6yqkzJbavHlJK9Jku6+uaouT3JLkp1JzuvuB6dzvTbJRUkOSfKJaUuS9yf5YFVtzWzmefMCrwcAAFIH2oTtpk2besuWLas9DAAA1rCqur67Ny13zDcRAgDAAAEaAAAGCNAAADBgkTcRHlBOfuMlqz0EGHb9r7x6tYcAAPsdM9AAADBAgAYAgAECNAAADBCgAQBggAANAAADBGgAABggQAMAwAABGgAABgjQAAAwQIAGAIABAjQAAAwQoAEAYIAADQAAAwRoAAAYIEADAMAAARoAAAYI0AAAMECABgCAAQI0AAAMEKABAGCAAA0AAAMEaAAAGCBAAwDAAAEaAAAGCNAAADBAgAYAgAECNAAADBCgAQBggAANAAADBGgAABggQAMAwAABGgAABgjQAAAwQIAGAIABAjQAAAwQoAEAYIAADQAAAwRoAAAYIEADAMAAARoAAAYI0AAAMECABgCAAQI0AAAMEKABAGCAAA0AAAMEaAAAGCBAAwDAAAEaAAAGCNAAADBgYQG6qp5YVddV1Z9V1c1V9YtT/WlVdVVVfXF6PXyuz5uramtV3VZVL5mrn1xVN07H3llVNdWfUFWXTfVrq+qYRV0PAAAki52BfiDJi7r7uUlOSnJ6VZ2a5E1Jru7u45JcPb1PVR2fZHOSE5KcnuQ9VXXQdK73Jjk3yXHTdvpUPyfJPd19bJILkrxtgdcDAACLC9A98+3p7cHT1knOSHLxVL84yZnT/hlJLu3uB7r7S0m2Jjmlqo5Kcmh3X9PdneSSXfosneuKJKctzU4DAMAiLHQNdFUdVFU3JLkryVXdfW2SZ3T39iSZXp8+Nd+Q5Pa57tum2oZpf9f6Q/p0984k9yY5YplxnFtVW6pqy44dO/bS1QEAcCBaaIDu7ge7+6QkGzObTT5xD82XmznuPdT31GfXcVzY3Zu6e9P69esfYdQAALB7++QpHN39zSR/lNna5TunZRmZXu+amm1LcvRct41J7pjqG5epP6RPVa1LcliSuxdxDQAAkCz2KRzrq+qp0/4hSV6c5M+TXJnk7KnZ2Uk+Ou1fmWTz9GSNZ2d2s+B10zKP+6rq1Gl986t36bN0rrOSfHJaJw0AAAuxboHnPirJxdOTNB6X5PLu/lhVXZPk8qo6J8lXkrw8Sbr75qq6PMktSXYmOa+7H5zO9dokFyU5JMknpi1J3p/kg1W1NbOZ580LvB4AAFhcgO7uLyR53jL1byQ5bTd9zk9y/jL1LUketn66u+/PFMABAGBf8E2EAAAwQIAGAIABAjQAAAwQoAEAYIAADQAAAwRoAAAYIEADAMAAARoAAAYI0AAAMECABgCAAQI0AAAMEKABAGCAAA0AAAMEaAAAGCBAAwDAAAEaAAAGCNAAADBAgAYAgAECNAAADBCgAQBggAANAAADBGgAABggQAMAwAABGgAABgjQAAAwQIAGAIABAjQAAAwQoAEAYIAADQAAAwRoAAAYIEADAMAAARoAAAYI0AAAMECABgCAAQI0AAAMEKABAGCAAA0AAAMEaAAAGCBAAwDAAAEaAAAGCNAAADBAgAYAgAECNAAADBCgAQBggAANAAADBGgAABggQAMAwAABGgAABgjQAAAwQIAGAIABAjQAAAwQoAEAYIAADQAAAwRoAAAYsLAAXVVHV9WnqurWqrq5qn5mqr+lqr5aVTdM20vn+ry5qrZW1W1V9ZK5+slVdeN07J1VVVP9CVV12VS/tqqOWdT1AABAstgZ6J1J3tDdP5jk1CTnVdXx07ELuvukaft4kkzHNic5IcnpSd5TVQdN7d+b5Nwkx03b6VP9nCT3dPexSS5I8rYFXg8AACwuQHf39u7+/LR/X5Jbk2zYQ5czklza3Q9095eSbE1ySlUdleTQ7r6muzvJJUnOnOtz8bR/RZLTlmanAQBgEfbJGuhpacXzklw7lV5XVV+oqg9U1eFTbUOS2+e6bZtqG6b9XesP6dPdO5Pcm+SIZT7/3KraUlVbduzYsXcuCgCAA9LCA3RVPTnJbyd5fXd/K7PlGM9JclKS7UnevtR0me69h/qe+jy00H1hd2/q7k3r168fuwAAAJiz0ABdVQdnFp4/1N0fSZLuvrO7H+zu7yR5X5JTpubbkhw9131jkjum+sZl6g/pU1XrkhyW5O7FXA0AACz2KRyV5P1Jbu3uX5urHzXX7GVJbpr2r0yyeXqyxrMzu1nwuu7enuS+qjp1Ouerk3x0rs/Z0/5ZST45rZMGAICFWLfAc78gyauS3FhVN0y1n03yyqo6KbOlFl9O8pok6e6bq+ryJLdk9gSP87r7wanfa5NclOSQJJ+YtmQW0D9YVVszm3nevMDrAQCAxQXo7v5sll+j/PE99Dk/yfnL1LckOXGZ+v1JXv4ohgkAAEN8EyEAAAwQoAEAYIAADQAAAwRoAAAYIEADAMAAARoAAAYI0AAAMECABgCAAQI0AAAMEKABAGCAAA0AAAMEaAAAGCBAAwDAAAEaAAAGCNAAADBAgAYAgAECNAAADBCgAQBggAANAAADBGgAABggQAMAwAABGgAABgjQAAAwQIAGAIABAjQAAAwQoAEAYIAADQAAAwRoAAAYIEADAMAAARoAAAYI0AAAMECABgCAAQI0AAAMEKABAGCAAA0AAAMEaAAAGCBAAwDAAAEaAAAGCNAAADBAgAYAgAECNAAADBCgAQBggAANAAADBGgAABggQAMAwAABGgAABgjQAAAwQIAGAIABAjQAAAwQoAEAYIAADQAAAwRoAAAYIEADAMCAhQXoqjq6qj5VVbdW1c1V9TNT/WlVdVVVfXF6PXyuz5uramtV3VZVL5mrn1xVN07H3llVNdWfUFWXTfVrq+qYRV0PAAAki52B3pnkDd39g0lOTXJeVR2f5E1Jru7u45JcPb3PdGxzkhOSnJ7kPVV10HSu9yY5N8lx03b6VD8nyT3dfWySC5K8bYHXAwAAiwvQ3b29uz8/7d+X5NYkG5KckeTiqdnFSc6c9s9Icml3P9DdX0qyNckpVXVUkkO7+5ru7iSX7NJn6VxXJDltaXYaAAAWYZ+sgZ6WVjwvybVJntHd25NZyE7y9KnZhiS3z3XbNtU2TPu71h/Sp7t3Jrk3yRHLfP65VbWlqrbs2LFjL10VAAAHooUH6Kp6cpLfTvL67v7WnpouU+s91PfU56GF7gu7e1N3b1q/fv0jDRkAAHZroQG6qg7OLDx/qLs/MpXvnJZlZHq9a6pvS3L0XPeNSe6Y6huXqT+kT1WtS3JYkrv3/pUAAMDMIp/CUUnen+TW7v61uUNXJjl72j87yUfn6punJ2s8O7ObBa+blnncV1WnTud89S59ls51VpJPTuukAQBgIdYt8NwvSPKqJDdW1Q1T7WeTvDXJ5VV1TpKvJHl5knT3zVV1eZJbMnuCx3nd/eDU77VJLkpySJJPTFsyC+gfrKqtmc08b17g9QAAwOICdHd/NsuvUU6S03bT5/wk5y9T35LkxGXq92cK4AAAsC/4JkIAABggQAMAwAABGgAABgjQAAAwQIAGAIABAjQAAAwQoAEAYIAADQAAA1YUoKvq6pXUAADgsW6P30RYVU9M8qQkR1bV4fnuNwsemuSZCx4bAACsOY/0Vd6vSfL6zMLy9flugP5WkncvblgAALA27TFAd/c7kryjqv5Vd79rH40JAADWrEeagU6SdPe7qupHkhwz36e7L1nQuAAAYE1aUYCuqg8meU6SG5I8OJU7iQANAMABZUUBOsmmJMd3dy9yMAAAsNat9DnQNyX5vkUOBAAA9gcrnYE+MsktVXVdkgeWit39TxYyKgAAWKNWGqDfsshBAADA/mKlT+H49KIHAgAA+4OVPoXjvsyeupEkj09ycJK/7u5DFzUwAABYi1Y6A/2U+fdVdWaSUxYxIAAAWMtW+hSOh+ju/yvJi/buUAAAYO1b6RKOfzr39nGZPRfaM6EBADjgrPQpHP94bn9nki8nOWOvjwYAANa4la6B/ulFDwQAAPYHK1oDXVUbq+p3ququqrqzqn67qjYuenAAALDWrPQmwl9PcmWSZybZkOT/nmoAAHBAWWmAXt/dv97dO6ftoiTrFzguAABYk1YaoL9eVT9VVQdN208l+cYiBwYAAGvRSgP0v0jyiiRfS7I9yVlJ3FgIAMABZ6WPsfvlJGd39z1JUlVPS/KrmQVrAAA4YKx0BvrvLoXnJOnuu5M8bzFDAgCAtWulAfpxVXX40ptpBnqls9cAAPCYsdIQ/PYkn6uqKzL7Cu9XJDl/YaMCAIA1aqXfRHhJVW1J8qIkleSfdvctCx0ZAACsQStehjEFZqEZAIAD2krXQAMAABGgAQBgiAANAAADBGgAABggQAMAwAABGgAABgjQAAAwQIAGAIABAjQAAAwQoAEAYIAADQAAAwRoAAAYIEADAMAAARoAAAYI0AAAMECABgCAAQI0AAAMWFiArqoPVNVdVXXTXO0tVfXVqrph2l46d+zNVbW1qm6rqpfM1U+uqhunY++sqprqT6iqy6b6tVV1zKKuBQAAlixyBvqiJKcvU7+gu0+ato8nSVUdn2RzkhOmPu+pqoOm9u9Ncm6S46Zt6ZznJLmnu49NckGSty3qQgAAYMnCAnR3fybJ3StsfkaSS7v7ge7+UpKtSU6pqqOSHNrd13R3J7kkyZlzfS6e9q9IctrS7DQAACzKaqyBfl1VfWFa4nH4VNuQ5Pa5Ntum2oZpf9f6Q/p0984k9yY5YpEDBwCAfR2g35vkOUlOSrI9ydun+nIzx72H+p76PExVnVtVW6pqy44dO4YGDAAA8/ZpgO7uO7v7we7+TpL3JTllOrQtydFzTTcmuWOqb1ym/pA+VbUuyWHZzZKR7r6wuzd196b169fvrcsBAOAAtE8D9LSmecnLkiw9oePKJJunJ2s8O7ObBa/r7u1J7quqU6f1za9O8tG5PmdP+2cl+eS0ThoAABZm3aJOXFW/leSFSY6sqm1JfiHJC6vqpMyWWnw5yWuSpLtvrqrLk9ySZGeS87r7welUr83siR6HJPnEtCXJ+5N8sKq2ZjbzvHlR1wIAAEsWFqC7+5XLlN+/h/bnJzl/mfqWJCcuU78/ycsfzRgBAGCUbyIEAIABAjQAAAwQoAEAYIAADQAAAwRoAAAYIEADAMAAARoAAAYI0AAAMECABgCAAQI0AAAMEKABAGCAAA0AAAMEaAAAGCBAAwDAAAEaAAAGCNAAADBAgAYAgAECNAAADBCgAQBggAANAAADBGgAABggQAMAwAABGgAABgjQAAAwQIAGAIABAjQAAAwQoAEAYIAADQAAAwRoAAAYIEADAMAAARoAAAYI0AAAMECABgCAAQI0AAAMEKABAGCAAA0AAAMEaAAAGCBAAwDAAAEaAAAGCNAAADBAgAYAgAECNAAADFi32gMAWImv/NIPrfYQ4HvyP/z8jas9BGAvMwMNAAADBGgAABggQAMAwAABGgAABgjQAAAwQIAGAIABAjQAAAwQoAEAYIAADQAAAwRoAAAYIEADAMCAhQXoqvpAVd1VVTfN1Z5WVVdV1Ren18Pnjr25qrZW1W1V9ZK5+slVdeN07J1VVVP9CVV12VS/tqqOWdS1AADAkkXOQF+U5PRdam9KcnV3H5fk6ul9qur4JJuTnDD1eU9VHTT1eW+Sc5McN21L5zwnyT3dfWySC5K8bWFXAgAAk4UF6O7+TJK7dymfkeTiaf/iJGfO1S/t7ge6+0tJtiY5paqOSnJod1/T3Z3kkl36LJ3riiSnLc1OAwDAouzrNdDP6O7tSTK9Pn2qb0hy+1y7bVNtw7S/a/0hfbp7Z5J7kxyx3IdW1blVtaWqtuzYsWMvXQoAAAeitXIT4XIzx72H+p76PLzYfWF3b+ruTevXr/8ehwgAAPs+QN85LcvI9HrXVN+W5Oi5dhuT3DHVNy5Tf0ifqlqX5LA8fMkIAADsVfs6QF+Z5Oxp/+wkH52rb56erPHszG4WvG5a5nFfVZ06rW9+9S59ls51VpJPTuukAQBgYdYt6sRV9VtJXpjkyKraluQXkrw1yeVVdU6SryR5eZJ0981VdXmSW5LsTHJedz84neq1mT3R45Akn5i2JHl/kg9W1dbMZp43L+paAABgycICdHe/cjeHTttN+/OTnL9MfUuSE5ep358pgAMAwL6yVm4iBACA/YIADQAAAwRoAAAYIEADAMAAARoAAAYI0AAAMECABgCAAQI0AAAMEKABAGCAAA0AAAMEaAAAGCBAAwDAAAEaAAAGCNAAADBAgAYAgAECNAAADBCgAQBggAANAAADBGgAABggQAMAwAABGgAABgjQAAAwQIAGAIABAjQAAAwQoAEAYIAADQAAAwRoAAAYIEADAMAAARoAAAYI0AAAMECABgCAAQI0AAAMEKABAGCAAA0AAAMEaAAAGCBAAwDAAAEaAAAGCNAAADBAgAYAgAECNAAADBCgAQBggAANAAADBGgAABggQAMAwAABGgAABgjQAAAwQIAGAIABAjQAAAwQoAEAYIAADQAAAwRoAAAYIEADAMAAARoAAAasSoCuqi9X1Y1VdUNVbZlqT6uqq6rqi9Pr4XPt31xVW6vqtqp6yVz95Ok8W6vqnVVVq3E9AAAcOFZzBvpHu/uk7t40vX9Tkqu7+7gkV0/vU1XHJ9mc5IQkpyd5T1UdNPV5b5Jzkxw3bafvw/EDAHAAWktLOM5IcvG0f3GSM+fql3b3A939pSRbk5xSVUclObS7r+nuTnLJXB8AAFiI1QrQneQPqur6qjp3qj2ju7cnyfT69Km+Icntc323TbUN0/6u9YepqnOraktVbdmxY8devAwAAA4061bpc1/Q3XdU1dOTXFVVf76Htsuta+491B9e7L4wyYVJsmnTpmXbAADASqzKDHR33zG93pXkd5KckuTOaVlGpte7pubbkhw9131jkjum+sZl6gAAsDD7PEBX1d+qqqcs7Sf5sSQ3JbkyydlTs7OTfHTavzLJ5qp6QlU9O7ObBa+blnncV1WnTk/fePVcHwAAWIjVWMLxjCS/Mz1xbl2S3+zu36uqP01yeVWdk+QrSV6eJN19c1VdnuSWJDuTnNfdD07nem2Si5IckuQT0wYAAAuzzwN0d/9lkucuU/9GktN20+f8JOcvU9+S5MS9PUYAANidtfQYOwAAWPMEaAAAGCBAAwDAAAEaAAAGCNAAADBAgAYAgAECNAAADBCgAQBggAANAAADVuOrvAGANegF73rBag8Bhv3xv/rjff6ZZqABAGCAAA0AAAMEaAAAGCBAAwDAAAEaAAAGCNAAADBAgAYAgAECNAAADBCgAQBggAANAAADBGgAABggQAMAwAABGgAABgjQAAAwQIAGAIABAjQAAAwQoAEAYIAADQAAAwRoAAAYIEADAMAAARoAAAYI0AAAMECABgCAAQI0AAAMEKABAGCAAA0AAAMEaAAAGCBAAwDAAAEaAAAGCNAAADBAgAYAgAECNAAADBCgAQBggAANAAADBGgAABggQAMAwAABGgAABgjQAAAwQIAGAIABAjQAAAwQoAEAYIAADQAAAwRoAAAYIEADAMCA/T5AV9XpVXVbVW2tqjet9ngAAHhs268DdFUdlOTdSX48yfFJXllVx6/uqAAAeCzbrwN0klOSbO3uv+zu/57k0iRnrPKYAAB4DKvuXu0xfM+q6qwkp3f3/zq9f1WSH+7u1+3S7twk505vfyDJbft0oDxaRyb5+moPAh7j/J7BYvkd2/88q7vXL3dg3b4eyV5Wy9Qe9j+C7r4wyYWLHw6LUFVbunvTao8DHsv8nsFi+R17bNnfl3BsS3L03PuNSe5YpbEAAHAA2N8D9J8mOa6qnl1Vj0+yOcmVqzwmAAAew/brJRzdvbOqXpfk95MclOQD3X3zKg+Lvc/yG1g8v2ewWH7HHkP265sIAQBgX9vfl3AAAMA+JUADAMAAAZo1y9e0w2JV1Qeq6q6qumm1xwKPVVV1dFV9qqpuraqbq+pnVntMPHrWQLMmTV/T/l+T/KPMHlf4p0le2d23rOrA4DGkqv5hkm8nuaS7T1zt8cBjUVUdleSo7v58VT0lyfVJzvTv2f7NDDRrla9phwXr7s8kuXu1xwGPZd29vbs/P+3fl+TWJBtWd1Q8WgI0a9WGJLfPvd8Wf+EAsB+rqmOSPC/Jtas8FB4lAZq1akVf0w4A+4OqenKS307y+u7+1mqPh0dHgGat8jXtADwmVNXBmYXnD3X3R1Z7PDx6AjRrla9pB2C/V1WV5P1Jbu3uX1vt8bB3CNCsSd29M8nS17TfmuRyX9MOe1dV/VaSa5L8QFVtq6pzVntM8Bj0giSvSvKiqrph2l662oPi0fEYOwAAGGAGGgAABgjQAAAwQIAGAIABAjQAAAwQoAEAYIAADQAAAwRogFVWVZ9bQZvXV9WTFjyOM6vq+Edoc1FVnbWXP3evnxNgkQRogFXW3T+ygmavTzIUoKvqoMGhnJlkjwEaAAEaYNVV1ben1xdW1R9V1RVV9edV9aGa+ddJnpnkU1X1qantj1XVNVX1+ar6cFU9eap/uap+vqo+m+Tle2j31qq6paq+UFW/WlU/kuSfJPmV6ZvSnrOCcZ9cVZ+uquur6ver6qiq+sGqum6uzTFV9YXdtd/LP0qAfUKABlhbnpfZbPPxSb4/yQu6+51J7kjyo939o1V1ZJL/kOTF3f0/JtmS5N/OneP+7v77Sf5wuXZV9bQkL0tyQnf/3ST/qbs/l+TKJG/s7pO6+y/2NMiqOjjJu5Kc1d0nJ/lAkvO7+9Ykj6+q75+a/mSSy3fX/lH8nABWzbrVHgAAD3Fdd29Lkqq6IckxST67S5tTMwvYf1xVSfL4JNfMHb/sEdp9K8n9Sf7PqvrdJB/7Hsb5A0lOTHLVdO6Dkmyfjl2e5BVJ3ppZgP7JR2gPsF8RoAHWlgfm9h/M8n9PV5KruvuVuznHXz9Su6o6JclpSTYneV2SFw2Os5Lc3N3PX+bYZUk+XFUfSdLd/cWq+qE9tAfYr1jCAbB/uC/JU6b9P0nygqo6Nkmq6klV9beX6bNsu2kd9GHd/fHMlouctMxnPJLbkqyvqudP5z64qk5Ikmn5x4NJ/mO+Oxu+2/YA+xsBGmD/cGGST1TVp7p7R5J/nuS3phv0/iTJ39m1wx7aPSXJx6bap5P8m6nLpUneWFX/zyPdRNjd/z3JWUneVlV/luSGJPNPE7ksyU9ltpxjJe0B9hvV3as9BgAA2G+YgQYAgAFuIgTgYarq3UlesEv5Hd3966sxHoC1xBIOAAAYYAkHAAAMEKABAGCAAA0AAAMEaAAAGPD/AZvpynF1TAPCAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 842.4x595.44 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_vars = ['bathrooms','bedrooms','latitude','longitude','price']\n",
    "cat_vars = ['building_id','manager_id','display_address','street_address']\n",
    "text_vars = ['description','features']\n",
    "\n",
    "rcParams['figure.figsize'] = 11.7,8.27\n",
    "df['interest_level'] = df['interest_level'].apply(lambda x: 0 if x=='low' else 1 if x=='medium' else 2)\n",
    "# Check homogeneity of target values\n",
    "sns.countplot('interest_level', data=df)\n",
    "plt.title('Unbalanced Classes')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing & Feature Engineering\n",
    "\n",
    "### Feature Engineering\n",
    "\n",
    "After taking a look at the current features available in the dataset, we notice that there are some variables with which we cannot work just yet. So as a first step, before going into the modeling journey, we decide to do some feature engineering in order to extract some more value from the existing features and create new variables that might help us in the next steps. \n",
    "\n",
    "For this section, we used, as a reference, the notebook available in Kaggle called \"[TwoSigmaRenthop - Advanced Feature Engineering](https://www.kaggle.com/chriscc/twosigmarenthop-advanced-feature-engineering)\". This notebook helped us understand different ways to handle several of the variables present in the notebook to add more value to our project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Categorical Variables\n",
    "\n",
    "When taking a look into the dataset, we noticed that especially in the address columns, there is a lot of inconsistency. Therefore, we first did basic text cleaning to have a coherent formatting. Additionally, we dropped every row that contains descriptions instead of actuall addresses. These description were identified by searching for ! and * in the address columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "display_address\n",
      "Num. of deleted addresses: 221\n",
      "Num. of Unique Addresses after Transformation: 6006\n",
      "street_address\n",
      "Num. of deleted addresses: 1\n",
      "Num. of Unique Addresses after Transformation: 11243\n"
     ]
    }
   ],
   "source": [
    "addresses = ['display_address', 'street_address']\n",
    "\n",
    "for address in addresses:\n",
    "    print(address)\n",
    "    ''' delete rows that contain descriptions instead of actual addresses '''\n",
    "    address_delete = [] \n",
    "    for i in range(len(df)):\n",
    "        address_val = df[address][i]\n",
    "        if search('!' or '*', address_val):\n",
    "            address_delete.append(i)\n",
    "\n",
    "    df = df.drop(df.index[address_delete])\n",
    "    print(\"Num. of deleted addresses: {}\".format(\n",
    "        len(address_delete)))\n",
    "    \n",
    "    \n",
    "    ''' Data Cleaning '''\n",
    "    address_column = df[address]\n",
    "    address_column_transformed = ( address_column\n",
    "                                           .apply(str.upper)\n",
    "                                           .apply(lambda x: x.replace('WEST','W'))\n",
    "                                           .apply(lambda x: x.replace('EAST','E'))\n",
    "                                           .apply(lambda x: x.replace('STREET','ST'))\n",
    "                                           .apply(lambda x: x.replace('AVENUE','AVE'))\n",
    "                                           .apply(lambda x: x.replace('BOULEVARD','BLVD'))\n",
    "                                           .apply(lambda x: x.replace('.',''))\n",
    "                                           .apply(lambda x: x.replace(',',''))\n",
    "                                           .apply(lambda x: x.replace('&',''))\n",
    "                                           .apply(lambda x: x.replace('(',''))\n",
    "                                           .apply(lambda x: x.replace(')',''))\n",
    "                                           .apply(lambda x: x.strip())\n",
    "                                           .apply(lambda x: x.replace('FIRST','1'))\n",
    "                                           .apply(lambda x: x.replace('SECOND','2'))\n",
    "                                           .apply(lambda x: x.replace('THIRD','3'))\n",
    "                                           .apply(lambda x: x.replace('FOURTH','4'))\n",
    "                                           .apply(lambda x: x.replace('FIFTH','5'))\n",
    "                                           .apply(lambda x: x.replace('SIXTH','6'))\n",
    "                                           .apply(lambda x: x.replace('SEVENTH','7'))\n",
    "                                           .apply(lambda x: x.replace('EIGHTH','8'))\n",
    "                                           .apply(lambda x: x.replace('EIGTH','8'))\n",
    "                                           .apply(lambda x: x.replace('NINTH','9'))\n",
    "                                           .apply(lambda x: x.replace('TENTH','10'))\n",
    "                                           .apply(lambda x: x.replace('ELEVENTH','11'))\n",
    "                                         )\n",
    "\n",
    "    print(\"Num. of Unique Addresses after Transformation: {}\".format(\n",
    "        address_column_transformed.nunique()))\n",
    "\n",
    "    df[address] = address_column_transformed "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We decided to also count for some features how often they appear in the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count features\n",
    "display=df[\"display_address\"].value_counts()\n",
    "manager_id=df[\"manager_id\"].value_counts()\n",
    "building_id=df[\"building_id\"].value_counts()\n",
    "street=df[\"street_address\"].value_counts()\n",
    "\n",
    "df[\"display_count\"]=df[\"display_address\"].apply(lambda x:display[x])\n",
    "df[\"manager_count\"]=df[\"manager_id\"].apply(lambda x:manager_id[x])  \n",
    "df[\"building_count\"]=df[\"building_id\"].apply(lambda x:building_id[x])\n",
    "df[\"street_count\"]=df[\"street_address\"].apply(lambda x:street[x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, some numeric-categorial interactions are created with the price and building features by adding some aggregations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Numeric-categorical interactions (price and building)\n",
    "price_by_building = df.groupby('building_id')['price'].agg([np.min,np.max,np.mean]).reset_index()\n",
    "price_by_building.columns = ['building_id','min_price_by_building',\n",
    "                            'max_price_by_building','mean_price_by_building']\n",
    "df = pd.merge(df,price_by_building, how='left',on='building_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last step would be to encode all ordinal variables ('building_id','manager_id','display_address','street_address') so that the model can use them as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OE = OrdinalEncoder()\n",
    "for cat_var in cat_vars:\n",
    "    print (\"Ordinal Encoding %s\" % (cat_var))\n",
    "    df[cat_var]=OE.fit_transform(df[[cat_var]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text Variables\n",
    "Studies have shown that titles with excessive all caps and special characters give renters the impression \n",
    "that the listing is fraudulent – i.e. BEAUTIFUL###APARTMENT###CHELSEA.\n",
    "We therefore decided to count the number of special characters in the description since it might give a hint for the seriousness of the inquery and hence the interest level. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['num_of_#']=df.description.apply(lambda x:x.count('#'))\n",
    "df['num_of_!']=df.description.apply(lambda x:x.count('!'))\n",
    "df['num_of_$']=df.description.apply(lambda x:x.count('$'))\n",
    "df['num_of_*']=df.description.apply(lambda x:x.count('*'))\n",
    "df['num_of_>']=df.description.apply(lambda x:x.count('>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, new features were derived out of the description by checking whether a phone number or email address is stated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['has_phone'] = df['description'].apply(lambda x:re.sub('['+string.punctuation+']', '', x).split())\\\n",
    "        .apply(lambda x: [s for s in x if s.isdigit()])\\\n",
    "        .apply(lambda x: len([s for s in x if len(str(s))==10]))\\\n",
    "        .apply(lambda x: 1 if x>0 else 0)\n",
    "\n",
    "print('Has phone: ' + str(np.round(len(np.where(df['has_phone'] == 1)[0]) / len(df), 2)) + '%')\n",
    "print('Has no phone: '+ str(np.round(len(np.where(df['has_phone'] == 0)[0]) / len(df), 2)) + '%')\n",
    "\n",
    "df['has_email'] = df['description'].apply(lambda x: 1 if '@renthop.com' in x else 0)\n",
    "print('Has email: '+ str(np.round(len(np.where(df['has_email'] == 1)[0]) / len(df), 2))+ '%')\n",
    "print('Has no email: '+ str(np.round(len(np.where(df['has_email'] == 0)[0]) / len(df), 2))+ '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to deal with these long text description and to derive some meaningful information out of it, we did some basic text cleaning and then calculated the length, number of words and number of features in the features column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count length, num of words and features of description of features \n",
    "display_address_column = df['description']\n",
    "df['description'] = [functions.text_cleaner(x) for x in display_address_column]\n",
    "\n",
    "df['length_description'] = df['description'].apply(lambda x: len(x))\n",
    "df['num_words_description'] = df['description'].apply(lambda x: len(x.split(\" \")))\n",
    "\n",
    "df['num_features'] = df['features'].apply(len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, for the text features, we counted the appearance of a feature token in the whole dataset using the CountVectorizer from sklearn. For every inquery we then checked if the respective feature was mentioned in the features column or not via One Hot Encoding. To have not every feature in the vocabulary, we limited the size to max features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract tokens of features and OHE \n",
    "v = CountVectorizer(stop_words='english', max_features=50)\n",
    "x = v.fit_transform(df['features']\\\n",
    "                                     .apply(lambda x: \" \".join([\"_\".join(i.split(\" \")) for i in x])))\n",
    "\n",
    "df1 = pd.DataFrame(x.toarray(), columns=v.get_feature_names())\n",
    "df.drop('features', axis=1, inplace=True)\n",
    "df = df.join(df1.set_index(df.index))\n",
    "\n",
    "tokens_features = v.get_feature_names()\n",
    "print(tokens_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Date Variables\n",
    "\n",
    "In the dataset, we have one text variable that refers to the date the rental advertisement was published. In order to use this variable we have to convert it from text to actual datetime type. After we do this, we decide to get some more features from this variable, including the year, month, day of month, day of week, and hour of publication. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['created'] = pd.to_datetime(df['created'])\n",
    "df['created_year'] = df['created'].dt.year\n",
    "df['created_month'] = df['created'].dt.month\n",
    "df['created_day_of_month'] = df['created'].dt.day\n",
    "df['created_day_of_week'] = df['created'].dt.dayofweek\n",
    "df['created_hour'] = df['created'].dt.hour"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Image Variables\n",
    "\n",
    "As the image links doesn't provide any value for the model, we decided to first count the number of photos for every inquery. Also, interactions between number of photos and bedrooms and bathrooms were created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['num_photos'] = df['photos'].apply(len)\n",
    "df['photos_per_bedroom'] = df[['num_photos','bedrooms']].apply(lambda x: x[0]/x[1] if x[1]!=0 else 0, axis=1)\n",
    "df['photos_per_bathroom'] = df[['num_photos','bathrooms']].apply(lambda x: x[0]/x[1] if x[1]!=0 else 0, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Numerical Variables\n",
    "\n",
    "From the previous section, we noticed that we have several numerical variables, and after many of the transformations, we end up with even more of them. Because of this, we decide to create some logical interactions that might have an impact on our model. \n",
    "\n",
    "We decide to calculate the total numer of rooms, which is the bedrooms plus the bathrooms. We then decide to calculate the price per room, bedroom and bathroom, where we divide the price by the number of the previously mentioned values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['total_rooms'] = df['bathrooms'] + df['bedrooms']\n",
    "df['price_per_room'] = df[['price','total_rooms']].apply(lambda x: x[0]/x[1] if x[1] != 0 else 0, axis=1)\n",
    "df['price_per_bedroom'] = df[['price','bedrooms']].apply(lambda x: x[0]/x[1] if x[1] != 0 else 0, axis=1)\n",
    "df['price_per_bathroom'] = df[['price','bathrooms']].apply(lambda x: x[0]/x[1] if x[1] != 0 else 0, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also decide to do something similar like before, but this time we calculate the price per word in the word description, as well as the price per length of the description. Moreover, we do the same thing with the features and photos, by calculating the price by each of them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['price_per_word_description'] = df[['price','num_words_description']].apply(lambda x: x[0]/x[1] if x[1] != 0 else 0, axis=1)\n",
    "df['price_per_length_description'] = df[['price','length_description']].apply(lambda x: x[0]/x[1] if x[1] != 0 else 0, axis=1)\n",
    "df['price_per_feature'] = df[['price','num_features']].apply(lambda x: x[0]/x[1] if x[1] != 0 else 0, axis=1)\n",
    "df['price_per_photo'] = df[['price','num_photos']].apply(lambda x: x[0]/x[1] if x[1] != 0 else 0, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we also notice that we have the location of the rented properties. We know that the location usually has an impact on the real estate, so we decide to calculate the distances to some well-known areas of New York City, including the Central Park, Wall Street, and Times Square. In order to calculate the distances, we use a function that we previously defined, that takes the differences of the coordinates and converts the difference into kilometers, so it makes more sense to us. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "central_park_coordinates = (40.7799963,-73.970621)\n",
    "df['distance_to_central_park'] = df[['latitude','longitude']].apply(\n",
    "        lambda x: functions.calculate_distance_between_coordinates(central_park_coordinates,(x[0],x[1])), axis=1)\n",
    "\n",
    "wall_street_coordinates = (40.7059692,-74.0099558)\n",
    "df['distance_to_wall_street'] = df[['latitude','longitude']].apply(\n",
    "        lambda x: functions.calculate_distance_between_coordinates(wall_street_coordinates,(x[0],x[1])), axis=1)\n",
    "\n",
    "times_square_coordinates = (40.7567473,-73.9888876)\n",
    "df['distance_to_times_square'] = df[['latitude','longitude']].apply(\n",
    "        lambda x: functions.calculate_distance_between_coordinates(times_square_coordinates,(x[0],x[1])), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation of Features and Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Object columns dropped\"\"\"\n",
    "df = df.drop(['building_id', 'listing_id', 'description', 'created', 'created_year','display_address', 'manager_id', \n",
    "              'photos', 'street_address'], axis=1) \n",
    "\n",
    "# Convert target values into ordinal values \n",
    "\n",
    "df_corr = df.corr()\n",
    "df_corr_abs = np.abs(df_corr['interest_level'])\n",
    "\n",
    "df_corr_abs_sort = df_corr_abs.sort_values(ascending = False)\n",
    "print(df_corr_abs_sort)\n",
    "\n",
    "sns.set(rc={'figure.figsize':(15.7,10.27)})\n",
    "sns.heatmap(df.corr())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Performance & Hyper-Parameter Tuning\n",
    "\n",
    "### Splitting of the Dataset\n",
    "\n",
    "After having performed the feature engineering and decided that our dataset is ready to be modeled, we first have to split our dataset into training and testing set. For this case, we first separate our independent variables (X) from our target variable (y). Since we are going to be using cross-validation, we decide to split only into training and testing set, without manually splitting also into a validation set, which will be created automatically during the cross-validation. As a testing size, we decide to use 15% of the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(\"interest_level\", axis=1)\n",
    "y = df[\"interest_level\"]\n",
    "\n",
    "X_train, X_test, y_train , y_test = train_test_split(X, y, test_size=0.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After performing data engineering and creating new variables, we end up with 91 variables in total. For this reason, we decided the next step to be feature selection. To avoid overfitting, this procedure is performed only in the training set. It is worth mentioning that in this case, we did not define any parameter for the random forest.\n",
    "What we are using is selectFromModel object from sklearn. It will select only those features which importance is greater than the mean importance of all features. This method is considered to be straightforward, fast and accurate.\n",
    "The results that we will get are 31 features only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sel = SelectFromModel(RandomForestClassifier())\n",
    "sel.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "#To see which feature are important\n",
    "sel.get_support()\n",
    "\n",
    "\n",
    "#to get the number of important features\n",
    "selected_feat= X_train.columns[(sel.get_support())]\n",
    "len(selected_feat)\n",
    "\n",
    "\n",
    "#to get the name of feature selected\n",
    "print(selected_feat)\n",
    "\n",
    "\n",
    "X_train= X_train[selected_feat]\n",
    "X_test= X_test[selected_feat]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning  for the Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our best features, next step is optimizing the random forest model. Our idea was to test on a wide range of values for each hyperparameter, that is why we decided to use Random Search Cross Validation from scikit-learn. It is considered to perform better in large datasets and in general picks the best result more often than other method. The range of values can be seen in the created parameter grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Number of trees in random forest\n",
    "n_estimators = np.arange(start=100, stop=2001, step=10)\n",
    " #Number of features to consider at every split'''\n",
    "max_features = ['auto', 'sqrt']\n",
    " #Maximum number of levels in tree'''\n",
    "max_depth = np.arange(start=10, stop=111, step=5)\n",
    "#max_depth.append(None)\n",
    " #Minimum number of samples required to split a node'''\n",
    "min_samples_split = np.arange(start=2, stop=101)\n",
    " #Minimum number of samples required at each leaf node'''\n",
    "min_samples_leaf = np.arange(start=2, stop=101)\n",
    "#Method of selecting samples for training each tree'''\n",
    "bootstrap = [True, False]\n",
    "\n",
    "\n",
    "'''Create the random grid'''\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "\n",
    "\n",
    "'''First create the base model to tune'''\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "''' Use the random grid to search for best hyperparameters'''\n",
    "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 20, cv = 5, verbose=2, \n",
    "                               random_state=123, n_jobs = -1)\n",
    "\n",
    "'''Fit the random search model'''\n",
    "rf_random.fit(X_train, y_train)\n",
    "\n",
    "print('test')\n",
    "\n",
    "best_params = rf_random.best_params_\n",
    "\n",
    "print(best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pruning of the Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to create another model: Decision Tree. Firstly, a decision tree is created by using gini as a criterion. Obviously, we will have overfitting because the model will be trained in its maximum depth. That is why now we go to the second step, pruning the tree. To prune the tree, we need to find the best alpha, which describes how much or how little the tree should be pruned, by deleting unnecessary branches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = DecisionTreeClassifier(criterion = \"gini\", random_state = 123) \n",
    "tree = tree.fit(X_train, y_train) \n",
    "y_pred = tree.predict(X_test) \n",
    "print (\"Accuracy : \", accuracy_score(y_test,y_pred)*100) \n",
    "\n",
    "\n",
    "\n",
    "#ploting the tree\n",
    "\n",
    "from sklearn.tree import plot_tree\n",
    "plt.figure(figsize = (15,17))\n",
    "plot_tree(tree, filled = True, rounded = True, class_names = ['Low Interest', 'Medium Interest', 'High Interest'], feature_names = X.columns)[0]\n",
    "\n",
    "#determine values for alpha\n",
    "path = tree.cost_complexity_pruning_path(X_train, y_train)\n",
    "\n",
    "#extract different values for alpha\n",
    "ccp_alphas = path.ccp_alphas \n",
    "ccp_alphas = ccp_alphas[:-1]\n",
    "\n",
    "\n",
    "#create an array to store the resuls of each fold during cross validation\n",
    "alpha_loop_values = []\n",
    "\n",
    "# For each candidate value alpha, we will run 5-fold cross validation\n",
    "# Then we will store the mean and std of the scores(accuracy) for each call\n",
    "# to cross_val_score in alpha_loop_values...\n",
    "\n",
    "for ccp_alpha in ccp_alphas:\n",
    "    tree = DecisionTreeClassifier(random_state=0, ccp_alpha=ccp_alpha)\n",
    "    scores = cross_val_score(tree, X_train, y_train, cv=5)\n",
    "    alpha_loop_values.append([ccp_alpha, np.mean(scores), np.std(scores)])\n",
    "#Now we can draw a graph of the means and std of the scores\n",
    "#for each candidate value for alpha\n",
    "alpha_results = pd.DataFrame(alpha_loop_values, \n",
    "                             columns=['alpha', 'mean_accuracy', 'std'])\n",
    "\n",
    "alpha_results.plot(x='alpha',\n",
    "                   y='mean_accuracy',\n",
    "                   yerr = 'std',\n",
    "                   marker = 'o',\n",
    "                  linestyle ='--')\n",
    "\n",
    "\n",
    "#Printing the best alpha\n",
    "\n",
    "column = alpha_results[\"mean_accuracy\"]\n",
    "max_index = column.idxmax()\n",
    "max_index\n",
    "\n",
    "\n",
    "ideal_ccp_alpha = alpha_results.iloc[max_index]['alpha']\n",
    "#ideal_ccp_alpha \n",
    "#Prunning the classification tree\n",
    "\n",
    "tree_pruned = DecisionTreeClassifier(random_state = 123,\n",
    "                                    ccp_alpha=ideal_ccp_alpha)\n",
    "tree_pruned = tree_pruned.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "y_pred = tree_pruned.predict(X_test) \n",
    "print (\"Accuracy : \", accuracy_score(y_test,y_pred)*100) \n",
    "\n",
    "#plotting the best tree\n",
    "plt.figure(figsize = (20,17))\n",
    "plot_tree(tree_pruned, filled = True, rounded = True,class_names = ['Low Interest', 'Medium Interest', 'High Interest'], feature_names = X.columns)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have both our models, the function below will take the results, and print the best model. As we can see, our best model is random forest with (TO BE COMPLETED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining model parameters from the tuned parameter\n",
    "model_params = {\n",
    "    'random_forest': {\n",
    "        'model': RandomForestClassifier(),\n",
    "        'params': {\n",
    "            'n_estimators': best_params['n_estimators'],\n",
    "            'min_samples_split': best_params['min_samples_split'],\n",
    "            'min_samples_leaf': best_params['min_samples_leaf'],\n",
    "            'max_features': best_params['max_features'],\n",
    "            'max_depth': best_params['max_depth'],\n",
    "            'bootstrap': best_params['bootstrap']\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    'decision_tree': {\n",
    "        'model': DecisionTreeClassifier(),\n",
    "        'params': {\n",
    "            'ccp_alpha': ideal_ccp_alpha\n",
    "        }\n",
    "    },\n",
    "    \n",
    "}\n",
    "\n",
    "#Loop over both decision tree and random forest to determine the best model for the given dataset\n",
    "scores = []\n",
    "print(model_params.items())\n",
    "\n",
    "trained_models = []\n",
    "for model_name, mp in model_params.items():\n",
    "    clf = mp['model']\n",
    "    clf.set_params(**mp['params'])\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict_proba(X_test)\n",
    "    scores.append({\n",
    "        'model': model_name,\n",
    "        'train_score': clf.score(X_train, y_train),\n",
    "        'test_score': clf.score(X_test, y_test),\n",
    "        'log_loss': log_loss(y_test, y_pred)\n",
    "    })\n",
    "    trained_models.append(clf)\n",
    "    \n",
    "best_model = pd.DataFrame(scores, columns= ['model','train_score','test_score', 'log_loss'])\n",
    "print(best_model)\n",
    "\n",
    "rf = trained_models[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction Explanation & Story Telling\n",
    "\n",
    "Now that we have our best model, we will get to know it more. In this section will be covered Gini Importance or Mean Decrease in Impurity (MDI), Permutation Importance or Mean Decrease in Accuracy, Partial Dependency Pots and SHAP values.\n",
    "\n",
    "### Feature Importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature importance shows what variables most affect predictions. The code below will perform both MDI and MDA. The first one will show us how effective the feature is at reducing uncertainty when creating a decision tree. From the other side, MDA will permutate feature data and then test how much that affects the model accuracy. If changing the dataset drops accuracy, then the feature is considered to be important.\n",
    "Results for this will be seen in the plots and final result table that we ploting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MDI\n",
    "MDI_importances = rf.feature_importances_\n",
    "indices = np.argsort(MDI_importances)\n",
    "features = X_train.columns\n",
    "\n",
    "#MDA\n",
    "MDA_test = permutation_importance(rf, X_test, y_test, n_repeats=10, random_state=123)\n",
    "sorted_idx1 = MDA_test.importances_mean.argsort()\n",
    "MDA_train = permutation_importance(rf, X_train, y_train, n_repeats=10, random_state=123)\n",
    "sorted_idx2 = MDA_train.importances_mean.argsort()\n",
    "\n",
    "#Plotting\n",
    "plt.rcParams[\"figure.figsize\"]=15,5\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3)\n",
    "ax1.set_title('MDI Importances')\n",
    "ax1.barh(range(len(indices)), MDI_importances[indices], color='b', align='center')\n",
    "ax1.set_yticks( np.arange(len(X_train.columns)))\n",
    "ax1.set_yticklabels(features[indices])\n",
    "ax1.set(xlabel='Relative Importance')\n",
    "ax2.boxplot(MDA_train.importances[sorted_idx2].T,\n",
    "vert=False, labels=features[sorted_idx2])\n",
    "tpmp=ax2.set_title(\"Permutation Importances (train)\")\n",
    "ax3.boxplot(MDA_test.importances[sorted_idx1].T,\n",
    "vert=False, labels=features[sorted_idx1])\n",
    "tpmp=ax3.set_title(\"Permutation Importances (test)\")\n",
    "fig.tight_layout()\n",
    "\n",
    "#Table of results\n",
    "# record times for comparison\n",
    "t0 = time()\n",
    "# Gini importance\n",
    "gini_imp = pd.DataFrame({'Feature': X_train.columns, 'Gini Importance': rf.feature_importances_}).set_index('Feature')\n",
    "\n",
    "t1 = time()# Permutation importance for train\n",
    "perm_imp = importances(rf, X_train, y_train)\n",
    "\n",
    "t2 = time()\n",
    "res= gini_imp.merge(perm_imp, left_index=True, right_index=True).reset_index().\\\n",
    "        rename(columns={'Importance': 'Permutation Importance'})\n",
    "res.loc[len(X_train.columns)+1] = ['runtime(s)', t1-t0, t2-t1]\n",
    "\n",
    "print(res)\n",
    "\n",
    "top5_feat = res[:-1].sort_values(by=['Permutation Importance'], ascending=False).head(5)\n",
    "print('Top 5 features with higher importance:', top5_feat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partial Dependence Plots\n",
    "\n",
    "Partial dependence plots show how each feature affects the model's predictions. It is always calculated after the model has been trained. In our case, since we are dealing with multiclass classification, each feature will have three different plots, one for each class. Because we have a lot of feature, we will plot and interpret PDPD plots for top 5 important features only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ploting_pdp(f=top5_feat.iloc[0,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "explain result 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ploting_pdp(f=top5_feat.iloc[1,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "explain result 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ploting_pdp(f=top5_feat.iloc[2,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "explain result 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ploting_pdp(f=top5_feat.iloc[3,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "explain result 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ploting_pdp(f=top5_feat.iloc[4,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "explain result 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PDPs assume no correlation between independent variables, but we might be curious about interactions between features. Here is when two-dimensional partial dependence plot is useful. What we will do next is that we will get top five correlated features and right after that, we will plot and interpret the results.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_top_abs_correlations(df, n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have top five correlated features, let's take a look at the plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot 2 dimension pdp for top 5 correlations\n",
    "features_to_plot = ['price', 'bedrooms'] #manually add the top 5 features\n",
    "two_dim_pdp(f=features_to_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exlain result 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot 2 dimension pdp for top 5 correlations\n",
    "features_to_plot = ['price', 'bedrooms']\n",
    "two_dim_pdp(f=features_to_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exlain result 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot 2 dimension pdp for top 5 correlations\n",
    "features_to_plot = ['price', 'bedrooms']\n",
    "two_dim_pdp(f=features_to_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exlain result 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot 2 dimension pdp for top 5 correlations\n",
    "features_to_plot = ['price', 'bedrooms']\n",
    "two_dim_pdp(f=features_to_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exlain result 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot 2 dimension pdp for top 5 correlations\n",
    "features_to_plot = ['price', 'bedrooms']\n",
    "two_dim_pdp(f=features_to_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exlain result 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SHAP Values measure the contribution each feature has in a machine learning model, in our case random forest. The show how much a given feature changed our prediction.\n",
    "\n",
    "First thing to plot is SHAP summary plot. It will summarize feature importance and what is driving it. Let's go through it using our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_summary(rf, X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explain results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explain what dependency contribution is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dependency_contribution(f=top5_feat.iloc[0,0], i=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After getting an overview of the model, we can go deeper by checking how a single feature contributes in our model. Here is when we use SHAP dependence contribution plot. They provide a similar insight to PDP's, but they add a lot more detail. For this reason, we will use the same features that we already used for PDP’s plots and compare the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dependency_contribution(f=top5_feat.iloc[0,0], i=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explain result 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dependency_contribution(f=top5_feat.iloc[0,0], i=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explain result 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dependency_contribution(f=top5_feat.iloc[1,0], i=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explain result 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dependency_contribution(f=top5_feat.iloc[1,0], i=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explain result 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dependency_contribution(f=top5_feat.iloc[1,0], i=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explain result 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last thing to be covered in this part are forceplots. They break down a prediction to show the impact of each feature and why our prediction was different from the baseline. In our case, we will randomly choose an instance and a class, and interpret the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = np.random.randint(1,df.shape[0]+1)\n",
    "m = np.random.randint(0,3)\n",
    "forceplots(n, m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explain the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extras\n",
    "\n",
    "### Neural Network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = len(np.unique(y))\n",
    "n_features = X_train.shape[1]\n",
    "\n",
    "ffnn = Sequential()\n",
    "ffnn.add(Dense(256, activation='relu', input_shape=(n_features,)))\n",
    "ffnn.add(Dropout(0.3))\n",
    "ffnn.add(Dense(64, activation='relu'))\n",
    "ffnn.add(Dense(32, activation='relu'))\n",
    "ffnn.add(Dense(n_classes, activation='softmax'))\n",
    "\n",
    "ffnn.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer=optimizers.Adam(lr=0.0001),\n",
    "              metrics=['accuracy'])\n",
    "ffnn.summary()\n",
    "\n",
    "trained_ffnn = ffnn.fit(X_train, y_train,\n",
    "          validation_data = (X_test, y_test),\n",
    "          epochs=50, \n",
    "          batch_size=8, \n",
    "          verbose=1\n",
    "         )\n",
    "\n",
    "rcParams['figure.figsize'] = 10, 5\n",
    "plt.plot(ffnn.history.history['loss'], label = \"Train Loss\")\n",
    "plt.plot(ffnn.history.history['val_loss'], label = \"Test Loss\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.legend()\n",
    "plt.plot()\n",
    "\n",
    "rcParams['figure.figsize'] = 10, 5\n",
    "plt.plot(ffnn.history.history['accuracy'], label = \"Train Accuracy\")\n",
    "plt.plot(ffnn.history.history['val_accuracy'], label = \"Test Accuracy\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.legend()\n",
    "plt.plot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
